{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook we demonstrate how to use learning models to correlate atomic structures with materials properties. We cover two standard linear methods: linear regression (LR) and principal components analysis (PCA). If you are already familiar with LR and PCA, you can continue onto [Principal Covariates Regression](2_PrincipalCovariatesRegression.ipynb).\n",
    "\n",
    "For each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. Second, we demonstrate a \"Utility Class\" for the models, which can be found in the utilities folder and contains all necessary functions.\n",
    "\n",
    "The input data is stored in a $n_{samples}\\times n_{features}$ feature matrix $\\mathbf{X}$, and the properties one might want to correlate structural features with in a $n_{samples}\\times n_{properties}$ matrix $\\mathbf{Y}$. \n",
    "\n",
    "Note that, as discussed in [Importing Data](X_ImportingData.ipynb), features and properties are centered and normalized. They have also been split into training and testing sets, denoted ($\\mathbf{X}_{train}$, $\\mathbf{Y}_{train}$) and ($\\mathbf{X}_{test}$, $\\mathbf{Y}_{test}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys, os\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append('../')\n",
    "from utilities.general import load_variables, get_stats\n",
    "from utilities.plotting import (\n",
    "    plot_projection, plot_regression, \n",
    "    plot_simple, \n",
    "    get_cmaps, table_from_dict,\n",
    "    check_mirrors\n",
    ")\n",
    "from utilities.kernels import linear_kernel, gaussian_kernel, center_kernel\n",
    "from utilities.classes import PCA, LR\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use('../utilities/kernel_pcovr.mplstyle')\n",
    "dbl_fig=(2*plt.rcParams['figure.figsize'][0], plt.rcParams['figure.figsize'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have these variables at our disposal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=%who_ls ndarray float64 list int str\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In principal component analysis (PCA), we seek to reduce the dimensionality of the input data $\\mathbf{X}$ such that the first few dimensions, which are easiest for humans to visualize, account for as much of the [variance](https://en.wikipedia.org/wiki/Variance) in the input data as possible. In this way, we reduce the dimensionality of the input data while retaining and enhancing the natural diversity in the data. In order to accomplish this, the input data are projected onto a subset of the eigenvectors of the covariance matrix of the data, namely the eigenvectors corresponding to the $n_{PCA}$ largest eigenvalues\n",
    "[(Pearson, 1901)](http://doi.org/10.1080/14786440109462720), \n",
    "[(Hotelling, 1936)](https://www.jstor.org/stable/2333955),\n",
    "[(Wikipedia)](https://en.wikipedia.org/wiki/Principal_component_analysis).\n",
    "\n",
    "<!---<span style=\"color:red\">RKC: We need some sort of image/graphic here</span> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction Error of a Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is the attempt to find an approximation of the $n_{samples}\\times n_{features}$ feature matrix in terms of a $n_{samples}\\times n_{PCA}$  **latent space** matrix, that we define as $\\mathbf{T} =\\mathbf{X} \\mathbf{P}_{XT}$.\n",
    "\n",
    "The matrix $\\mathbf{P}_{XT}$ projects between feature space and latent space. The approximation of the feature matrix in terms of latent space coordinates can be written in terms of another linear projection $\\mathbf{P}_{TX}$ as $\\mathbf{X}_{PCA} = \\mathbf{T} \\mathbf{P}_{TX}$. \n",
    "\n",
    "Requiring that  $\\mathbf{X}_{PCA} \\mathbf{P}_{XT} = \\mathbf{T} $ (i.e. that $\\mathbf{X}_{PCA}$ a perfect approximation of the portion of the feature space that is projected to the latent space) implies that $\\mathbf{P}_{TX}\\mathbf{P}_{XT}=\\mathbf{I}$. Without loss of generality we take the projection matrix to the latent space to be orthogonal, which implies that $\\mathbf{P}_{TX}=\\mathbf{P}_{XT}^T$.\n",
    "\n",
    "The **reconstruction error** is defined as the difference between the original feature matrix and that approximated using only latent-space information, \n",
    "\n",
    "\\begin{equation}\n",
    "\\ell=\\lVert\\mathbf{X}-\\mathbf{X}_{PCA}\\rVert^2  = \n",
    "\\lVert\\mathbf{X}-\\mathbf{X}\\mathbf{P}_{XT}\\mathbf{P}_{XT}^T\\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lVert \\cdot \\rVert$ denotes the [Frobenius norm](http://mathworld.wolfram.com/FrobeniusNorm.html). \n",
    "\n",
    "Using the orthogonality of $\\mathbf{P}_{XT}$, $\\mathbf{P}_{XT}^T\\mathbf{P}_{XT} = \\mathbf{I}$, and the [matrix relations](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf):\n",
    "\n",
    "\\begin{align}\n",
    "(1) & \\quad\\left\\lVert\\mathbf{A}\\right\\rVert^2=\\operatorname{Tr}(\\mathbf{A}\\mathbf{A}^T) \\\\\n",
    "(2) & \\quad\\left(\\mathbf{I}-\\mathbf{P}_{XT} \\mathbf{P}_{XT}^T\\right)^2 = \\left(\\mathbf{I}-\\mathbf{P}_{XT} \\mathbf{P}_{XT}^T\\right)\\\\\n",
    "(3) & \\quad\\operatorname{Tr}(\\mathbf{A B C}) = \\operatorname{Tr}(\\mathbf{B C A})\n",
    "\\end{align}\n",
    "\n",
    "we write\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell=\\operatorname{Tr}\\left(\\mathbf{X} \\left(\\mathbf{I}-\\mathbf{P}_{XT} \\mathbf{P}_{XT}^T\\right) \\mathbf{X}^T \\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding the optimal projection to the latent space amounts to minimizing the loss $\\ell$ within the space of orthogonal matrices. Using another time the fact that the trace of a product is invariant to circular permutations of the factors (Eq. (3) above), and given that  $\\operatorname{Tr}\\left(\\mathbf{X}\\mathbf{X}^T\\right)$ is a constant, minimizing $\\ell$ is equivalent to _maximising_ the similarity $\\rho$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho=\\operatorname{Tr}\\left(\\mathbf{P}_{XT}^T \\mathbf{X}^T \\mathbf{X} \\mathbf{P}_{XT}\\right).\n",
    "\\end{equation}\n",
    "\n",
    "By recognizing that the equation for $\\mathbf{\\rho}$ can be written in terms of a [Rayleigh Quotient](https://en.wikipedia.org/wiki/Rayleigh_quotient#Special_case_of_covariance_matrices), we note that $\\rho$ is maximised when $\\mathbf{P}_{XT}$ is the matrix built out of the $n_{PCA}$ principal eigenvectors of $\\mathbf{X}^T\\mathbf{X}=\\mathbf{C}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Covariance and Eigendecomposition\n",
    "\n",
    "To perform PCA in practice, we first compute the covariance matrix $\\mathbf{C}=\\mathbf{X}^T\\mathbf{X}$ and its eigendecomposition,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} = \\mathbf{U}_C \\mathbf{\\Lambda}_C \\mathbf{U}_C^T,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{\\Lambda}_C$ is a matrix with the eigenvalues of $\\mathbf{C}$ on the diagonal in decreasing order (top left to bottom right), and $\\mathbf{U}_C$ is a matrix containing the corresponding eigenvectors as columns.\n",
    "\n",
    "Note that, when divided by the number of samples, the inner product of the training feature matrices only corresponds to an estimator of the covariance of the data when the feature matrix has been previously centered, which it has been here. [(Wikipedia)](https://en.wikipedia.org/wiki/Covariance#Relationship_to_inner_products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.matmul(X_train.T, X_train)\n",
    "\n",
    "v_C, U_C = np.linalg.eigh(C)\n",
    "\n",
    "# U_C/v_C are already sorted, but in *increasing* order, so reverse them\n",
    "U_C = np.flip(U_C, axis=1)\n",
    "v_C = np.flip(v_C, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation and sorting of the eigenvalues can also be performed using the `sorted_eig(matrix, threshold, n)` function in `utilities/general.py`.\n",
    "\n",
    "Given that we have scaled the features to have unit variance, the magnitude of the eigenvalues, relative to the sum of the eigenvalues, indicates the fraction of the variance associated with each principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(np.asarray(range(len(v_C)))+1, v_C/n_train, marker='o')\n",
    "plt.title(r\"Size of $n^{th}$ Eigenvalue $v_n$\")\n",
    "plt.xlabel(\"n\"); plt.ylabel(r\"$v_n$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Low-Dimensional Projection and Approximation\n",
    "\n",
    "As discussed above, the projection matrix on the PCA latent space corresponds to the orthonormal basis defined by the eigenvectors $\\mathbf{P}_{XT}= \\hat{\\mathbf{U}}_C$, where $\\hat{\\mathbf{U}}_C$ includes only the first $n_{PCA}$ eigenvectors of the covariance ($\\mathbf{U}_C$) as columns and is therefore of size $n_{samples}\\times n_{PCA}$. The resulting projection yields the low-dimensional representation,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{X}\\hat{\\mathbf{U}}_C,\n",
    "\\end{equation}\n",
    "\n",
    "PCA projections give a simplified representation of the data that may (or may not!) correlate with the property of interest, but should include most of the variance in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Our low-dimensional space contains {n_PCA} components.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PXT = U_C[:,:n_PCA]\n",
    "PTX = PXT.T\n",
    "\n",
    "T = np.matmul(X_train, PXT)\n",
    "\n",
    "plot_projection(Y_train, T, **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, the low-dimensional projections can also be used to construct a low-rank approximant of the input space, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X}_{PCA} = \\mathbf{T} \\mathbf{P}_{XT}^T = \\mathbf{T}\\hat{\\mathbf{U}}_C^T,\n",
    "\\end{equation}\n",
    "\n",
    "i.e., vectors that have the same dimensionality of the initial features, but actually are just lying in a $n_{PCA}$-dimensional subspace. This yields our original projection equation $\\mathbf{X}_{PCA} = \\mathbf{X}\\mathbf{P}_{XT}\\mathbf{P}_{XT}^T$.\n",
    "\n",
    "In the plot below, we represent the first two components of the feature space for $\\mathbf{X}$ and $\\mathbf{X}_{PCA}$. Given the high dimensionality of the feature space, the figure cannot quantitatively represent the accuracy of the reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_PCA = np.matmul(T, PTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=dbl_fig)\n",
    "plot_simple(X_train, fig=fig, ax=ax[0], title=\"Original Data\", **cmaps)\n",
    "plot_simple(X_PCA, fig=fig, ax=ax[1], title=\"Best Reconstruction with {} PCs\".format(n_PCA), **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out-of-sample projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once PCA has been performed on a **training set** and the eigenvectors have been found, it is possible to find low-dimensional projections (and reconstructed approximants) of other data points (the **test set**), which is approximately optimal in the same sense as the PCA loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $\\mathbf{X}_{test}$ is the feature matrix of the test points, their latent-space projection can be obtained as $\\mathbf{T}_{test}=\\mathbf{X}_{test}\\mathbf{P}_{XT}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_PCA_test = np.matmul(X_test, PXT)\n",
    "X_test_PCA = np.matmul(T_PCA_test,PTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_projection(Y_test, T_PCA_test, **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute errors for the out-of-sample projections based on the same expression for the loss that we used to build the PCA projection for the training set:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell=\\lVert\\mathbf{X}_{test}-\\mathbf{X}_{PCA, test}\\rVert^2 = \\lVert\\mathbf{X}_{test}-\\mathbf{X}_{test}\\mathbf{P}_{XT}\\mathbf{P}_{TX}\\rVert^2 \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have supplied a utility `get_stats` and a markdown table converter for statistics in `utilities/plotting.py`, which can be called to compare or print statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([get_stats(x=X_train, y=Y_train, t=T, xr=X_PCA), \n",
    "                 get_stats(x=X_test, y=Y_test, t=T_PCA_test, xr=X_test_PCA)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may assume, the error in the projection decreases as we include more components. Here we show the decrease in the approximation error, and show how the first two features in $\\mathbf{X}_{PCA}$ converge to the exact value as the number of selected components approaches the number of features of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = range(X_test.shape[1])[::5]\n",
    "\n",
    "n_plot = 4\n",
    "fsize = (3*n_plot,3)\n",
    "\n",
    "fig, ax = plt.subplots(1, n_plot+1, figsize=fsize, sharex=True, sharey=True)\n",
    "\n",
    "for i,n in enumerate(trials[::int(len(trials)/n_plot)]):\n",
    "    n = max(n,2)\n",
    "    PXT = U_C[:,:n]\n",
    "    PTX = PXT.T\n",
    "    \n",
    "    X_r = np.matmul( np.matmul(X_test, PXT), PTX)\n",
    "    \n",
    "    ax[i].scatter(X_r[:,0], X_r[:,1], c=Y_test[:,0], linewidth=0.5, edgecolor='k')\n",
    "    ax[i].set_title(f\"{n} Comp.\")\n",
    "    ax[i].set_xlabel(r\"$X_1$\")\n",
    "\n",
    "ax[-1].scatter(X_test[:,0], X_test[:,1], c=Y_test[:,0], linewidth=0.5, edgecolor='k')\n",
    "ax[-1].set_title(\"Original Test Data\")\n",
    "ax[-1].set_xlabel(r\"$X_1$\")\n",
    "ax[0].set_ylabel(r\"$X_2$\")\n",
    "\n",
    "fig.suptitle(\"Reconstructed Data\", y=1.1)\n",
    "fig.subplots_adjust(wspace=0.1)\n",
    "\n",
    "plt.figure(figsize=fsize)\n",
    "plt.loglog(trials, \\\n",
    "           [((X_test-np.matmul(np.matmul(X_test, U_C[:, 0:n]), U_C[:,:n].T))**2).mean(axis=0).sum() for n in trials], \\\n",
    "           \"b-\", marker='o')\n",
    "plt.title(r\"Representation Error for n Components\")\n",
    "plt.xlabel(\"n\"); plt.ylabel(\"Error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical Multidimensional Scaling (MDS)\n",
    "\n",
    "A reduction in the dimension of the feature space is also achieved with a different logic that underlies several dimensionality reduction techniques known as [multidimensional scaling (MDS)](https://en.wikipedia.org/wiki/Multidimensional_scaling) [(Torgerson1952)](https://doi.org/10.1007/BF02288916).\n",
    "In MDS, the projected feature space is chosen to preserve the _pairwise distances_ of the original space, defining the loss (in MDS known as the $\\textit{strain}$):\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell = \\sum_{j}\\sum_{i<j} \\left[\\left( d(X_i,X_j) - d(T_i - T_j)\\right)^2\\right]\n",
    "\\label{eq:loss-mds-sum}\n",
    "\\end{equation}\n",
    "\n",
    "where $d$ is a distance measure and $X_i$ and $T_i$ refer to the $i^{th}$ samples in the full and projected feature matrices $\\mathbf{X}$ and $\\mathbf{T}$.\n",
    "\n",
    "In *classical MDS*, also known as Principal _Coordinates_ Analysis, the distance between features is the [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance). If we assume that $\\mathbf{X}$ is centered, i.e. $\\bar{X} = 0$ and construct a triangle between $X_i$, $X_j$, and 0, we use the law of cosines to retrieve the Euclidean distance:\n",
    "\n",
    "\\begin{equation}\n",
    "d^2(X_i,X_j) = \\lVert X_i \\rVert^2 + \\lVert X_j \\rVert^2 - 2 \\langle X_i, X_j\\rangle\n",
    "\\end{equation}\n",
    "\n",
    "Given that $ij^{th}$ element of the Gram matrix $\\mathbf{K} = \\mathbf{X} \\mathbf{X}^T$ contains the inner products of $X_i$ and $X_j$, we then write our distance in terms of $K_{ij}$.\n",
    "\n",
    "\\begin{equation}\n",
    "d^2(X_i,X_j) = K_{ii} + K_{jj} - 2 K_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "Therefore, there is a 1:1 mapping between the distance matrix $\\mathbf{D}$ and the Gram matrix $\\mathbf{K}$. This motivates the choice of a related (but not equivalent) loss function\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell = \\lVert \\mathbf{X}\\mathbf{X}^T - \\mathbf{T}\\mathbf{T}^T\\rVert^2,\n",
    "\\label{eq:loss-mds}\n",
    "\\end{equation}\n",
    "\n",
    "also known as Torgerson scaling, that aims to find the best low-rank approximation of the Gram matrix.\n",
    "\n",
    "The 1:1 relation between the Euclidean distance and the scalar product matrix means that if one finds a solution that zeroes the Torgerson loss, that solution also perfectly reproduces the distance matrix. However, approximate solutions to the Gram matrix matching problem are not necessarily local optima of the distance matrix matching problem. \n",
    "\n",
    "Further discussion of this derivation can be found in [The Elements of Statistical Learning: Data Mining, Inference, and Prediction](https://doi.org/10.1007/978-0-387-84858-7), [section 18.5.2](https://web.stanford.edu/~hastie/ElemStatLearn//printings/ESLII_print12.pdf). Also featured in this great [stackexchange discussion.](https://stats.stackexchange.com/questions/14002/whats-the-difference-between-principal-component-analysis-and-multidimensional) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.matmul(X_train, X_train.T)\n",
    "K = center_kernel(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Low-Dimensional Projection and Approximation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the eigenvalue decomposition of the Gram matrix reads $\\mathbf{K} = \\mathbf{U}_\\mathbf{K} \\mathbf{\\Lambda}_\\mathbf{K} \\mathbf{U}_\\mathbf{K}^T$, $\\ell$ is minimized when $\\mathbf{T}\\mathbf{T}^T$ is given by the singular value decomposition of $\\mathbf{K}$, that is by taking \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\hat{\\mathbf{U}}_\\mathbf{K} \\hat{\\mathbf{\\Lambda}}_\\mathbf{K}^{1/2}\n",
    "\\label{eq:tuk}\n",
    "\\end{equation}\n",
    "\n",
    "restricted to the largest $n_{MDS}$ eigenvectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_MDS = 2 # size of low-dimensional space\n",
    "\n",
    "v_K, U_K = np.linalg.eigh(K)\n",
    "\n",
    "# U_K/v_K are already sorted, but in *inKreasing* order, so reverse them\n",
    "U_K = np.flip(U_K, axis=1)\n",
    "v_K = np.flip(v_K, axis=0)\n",
    "\n",
    "U_K = U_K[:,v_K>0]\n",
    "v_K = v_K[v_K>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting projection is might seem a bit familiar (more on this in Sec. 3.3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.matmul(U_K[:, :n_MDS], \n",
    "              np.diagflat(np.sqrt(v_K[:n_MDS]))\n",
    "             )\n",
    "plot_projection(Y_train, T, title=\"Multidimensional Scaling of Training Data\", **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting New Data\n",
    "\n",
    "Given that our latent-space projection is not given as a function of the input space, how do we project new data? First we reconstruct $\\mathbf{T}$ as a function of $\\mathbf{X}$ by solving for $\\mathbf{P}_{XT}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{X}\\mathbf{P}_{XT}.\n",
    "\\end{equation}\n",
    "\n",
    "We solve by multiplying both sides by the [pseudo-inverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of $\\mathbf{X}$\n",
    "\n",
    "\\begin{equation}\n",
    "(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{T} = \\mathbf{P}_{XT}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PXT = np.linalg.solve(np.dot(X_train.T,X_train),np.dot(X_train.T,T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".... or as a least-square problem, which is faster and more stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PXT = np.linalg.lstsq(X_train, T, rcond=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = np.matmul(X_test, PXT)\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=dbl_fig)\n",
    "plot_projection(Y_train, T, fig=fig, ax=ax[0], title=\"Multidimensional Scaling of Training Data\", **cmaps)\n",
    "plot_projection(Y_test, T_test, fig=fig, ax=ax[1], title=\"Multidimensional Scaling of Testing Data\", **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection with PCA, and with the SVD of $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can take advantage of the relationship between the eigenvectors and eigenvalues of the covariance $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X}$ and those of the Gram matrix $\\mathbf{K} = \\mathbf{X}\\mathbf{X}^T$.\n",
    "\n",
    "In fact, it is possible to show that the singular-value decomposition of \n",
    "$\\mathbf{X}$ can be written as $\\mathbf{X}=\\mathbf{U_K}\\mathbf{\\Sigma}\\mathbf{U_C}^T$, where $\\mathbf{\\Sigma}$ is a rectangular matrix that is full of zeros, except for having the square root of the non-zero eigenvalues of $\\mathbf{C}$ or $\\mathbf{K}$  (that are the same) on the diagonal.\n",
    "\n",
    "Then, one sees that \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{U_K} = \\mathbf{X}\\mathbf{U_C}\\mathbf{\\Lambda_C}^{-1/2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.matmul(X_train.T, X_train)\n",
    "\n",
    "v_C, U_C = np.linalg.eigh(C)\n",
    "\n",
    "# U_C/v_C are already sorted, but in *increasing* order, so reverse them\n",
    "U_C = np.flip(U_C, axis=1)\n",
    "v_C = np.flip(v_C, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we see that the largest eigenvalues of $\\mathbf{C}$ and $\\mathbf{K}$ are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(v_K,\"b-\", marker='o', label=r'$\\Lambda_K$')\n",
    "plt.loglog(v_C,\"r-\", marker='o', label=r'$\\Lambda_C$')\n",
    "plt.title(r\"Size of $n^{th}$ Eigenvalue $v_n$\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"n\"); plt.ylabel(r\"$v_n$\");\n",
    "\n",
    "np.linalg.norm(v_K[:v_C.shape[0]]-v_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these relationships we obtain MDS\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T}=\\mathbf{\\hat{U}_K}\\mathbf{\\hat{\\Lambda}_K}^{1/2} = \\mathbf{X}\\mathbf{\\hat{U}_C}\\mathbf{\\hat{\\Lambda}_C}^{-1/2}\\mathbf{\\hat{\\Lambda}_K}^{1/2}= \\mathbf{X}\\hat{\\mathbf{U}}_\\mathbf{C}\n",
    "\\end{equation}\n",
    "\n",
    "which shows that MDS and PCA yield the same projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = np.matmul(X_test, PXT)\n",
    "\n",
    "ref = PCA(n_PCA=n_MDS)\n",
    "ref.fit(X_train)\n",
    "T_ref = ref.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=dbl_fig)\n",
    "plot_projection(Y_test, T, fig=fig, ax=ax[0], title=\"Multidimensional Scaling\", **cmaps)\n",
    "plot_projection(Y_test, check_mirrors(T_ref,T), fig=fig, ax=ax[1], title=\"PCA\", **cmaps)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the case of PCA, we see how the error decreases as the number of components is increased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = np.matmul(X_test, X_test.T)\n",
    "ns = range(X_train.shape[1])\n",
    "ns = [*ns[:20], *ns[20::10]]\n",
    "\n",
    "PXT = np.linalg.lstsq(X_train, np.matmul(U_K, np.diagflat(np.sqrt(v_K))), rcond=None)[0]\n",
    "T = np.matmul(X_test, PXT)\n",
    "strains = [(np.linalg.norm(K) - np.linalg.norm(np.matmul(T[:,:n], T[:,:n].T)))/np.linalg.norm(K) for n in ns]\n",
    "\n",
    "\n",
    "plt.semilogy(ns, strains, marker='o')\n",
    "plt.xlabel(\"Number of MDS Components\")\n",
    "plt.ylabel(\"Strain\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Weights which Minimize Loss\n",
    "In [linear regression](https://en.wikipedia.org/wiki/Linear_regression#Estimation_methods) (LR), we aim to find a linear relationship between the features (inputs) and target(s) that minimizes the error of the reconstruction of the target variable. In other words, we seek a model \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{Y}} = \\mathbf{X}\\mathbf{W},\n",
    "\\end{equation}\n",
    "\n",
    "where the weights $\\mathbf{W}$  minimize the loss,\n",
    "\n",
    "\\begin{align}\n",
    "\\ell &= \\left\\lVert \\mathbf{Y} - \\hat{\\mathbf{Y}}\\right\\rVert^2\\\\\n",
    "&=\\left\\lVert \\mathbf{Y} - \\mathbf{X}\\mathbf{W}\\right\\rVert^2.\n",
    "\\end{align}\n",
    "\n",
    "By minimizing the loss function with respect to $\\mathbf{W}$ using the [matrix relations](https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf)\n",
    "\n",
    "\\begin{align}\n",
    "(1) & \\quad\\left\\lVert\\mathbf{A}\\right\\rVert^2=\\operatorname{Tr}(\\mathbf{A}\\mathbf{A}^T) \\\\\n",
    "(2) & \\quad (\\mathbf{A}+\\mathbf{B})^T = \\mathbf{A}^T + \\mathbf{B}^T\\\\\n",
    "(3) & \\quad \\operatorname{Tr}(\\mathbf{A} + \\mathbf{B}) = \\operatorname{Tr}(\\mathbf{A}) + \\operatorname{Tr}(\\mathbf{B})\\\\\n",
    "(4) & \\quad\\operatorname{Tr}(\\mathbf{A B C}) = \\operatorname{Tr}(\\mathbf{B C A})\\\\\n",
    "(5) & \\quad \\frac{\\partial}{\\partial\\mathbf{B}} \\operatorname{Tr} (\\mathbf{AB}^T) = \\mathbf{A}\\\\\n",
    "(6) & \\quad \\frac{\\partial}{\\partial\\mathbf{B}} \\operatorname{Tr} (\\mathbf{ABB}^T\\mathbf{A}^T) = 2\\mathbf{A}^T\\mathbf{AB},\\\\\n",
    "\\end{align}\n",
    "\n",
    "one obtains\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W} = (\\mathbf{X}^T \\mathbf{X} + \\lambda \\mathbf{I})^{-1} \\mathbf{X}^T \\mathbf{Y}.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is the small regularization parameter. When $\\lambda$ is greater than zero, the model is known as *ridge regression*, corresponding to a $\\mathcal{L}^2$ regularization of the loss, i.e. a penalty that is associated with models with large weights\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell = \\left\\lVert \\mathbf{Y} - \\mathbf{XW}\\right\\rVert^2 + \\lambda \\left\\lVert \\mathbf{W}\\right\\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "We look at the effect of this regularization parameter in Section 4.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization=1e-16\n",
    "\n",
    "XTX = np.matmul(X_train.T, X_train)\n",
    "XTX = XTX + regularization*np.eye(X_train.shape[1])\n",
    "iXTX = np.linalg.pinv(XTX)\n",
    "XTY = np.matmul(X_train.T, Y_train)\n",
    "\n",
    "W_LR = np.matmul(iXTX,XTY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction for the train (or test) set $\\mathbf{X}$ is easily obtained multiplying the features by the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_lr_train = np.matmul(X_train, W_LR)\n",
    "Y_lr_test = np.matmul(X_test, W_LR)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(Y_train[:,0], Y_lr_train[:,0], title=\"Linear Regression: Training Set\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], Y_lr_test[:,0], title=\"Linear Regression: Testing Set\", fig=fig, ax=axes[1], **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "We then go back and calculate the loss functions from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([get_stats(x=X_train, y=Y_train, yp=Y_lr_train), \n",
    "                 get_stats(x=X_test, y=Y_test, yp=Y_lr_test)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"LR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing a Regularization\n",
    "\n",
    "Adding small amounts of noise to data in regressions leads to a more robust model [(Marquardt 1970)](https://www.tandfonline.com/doi/pdf/10.1080/00401706.1970.10488699?casa_token=ZAXSZfnnkGsAAAAA:aji-q3QyTuSZRkBTTx1k_Z4Fnc3YQimEvJHxSQPzfTwoF6ksV0tbXb06TmP08__yalTAkQpbEszS1co), [(Koistinen 1992)](http://papers.nips.cc/paper/459-kernel-regression-and-backpropagation-training-with-noise.pdf),  [(Gupta 2017)](https://towardsdatascience.com/regularization-in-machine-learning-76441ddcf99a) by reducing the [overfitting](https://en.wikipedia.org/wiki/Overfitting). \n",
    "A similar result is achieved by setting an appropriate value for the parameter $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularizations = np.linspace(-10, 0, 24)\n",
    "lrs = [LR(regularization=10**i) for i in regularizations]\n",
    "\n",
    "for lr in lrs:\n",
    "    lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff_deter = np.array([lr.statistics(X_test, Y_test)['Coefficient of Determination<br>($R^2$)'] for lr in lrs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even small values of the regularization lead to an increase in the fit of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(10**regularizations, coeff_deter, marker='o')\n",
    "\n",
    "best_regularization = 10**regularizations[coeff_deter.argmax()]\n",
    "plt.scatter([best_regularization], \n",
    "            [coeff_deter.max()], c='r', zorder=3)\n",
    "\n",
    "print(f\"The ideal value of $\\lambda$ for this LR (with respect to $R^2$) is {round(best_regularization,4)}.\")\n",
    "\n",
    "plt.xlabel(r\"$\\lambda$\")\n",
    "plt.ylabel(r\"$R^2$\")\n",
    "\n",
    "plt.title(r\"Effect of $\\lambda$ on $R^2$ for Linear Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Combining LR and PCA\n",
    "\n",
    "In the next notebook, we show how PCA and LR can be combined into a new model. [Continue on -->](2_PrincipalCovariatesRegression.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Utility Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes from the utility module enable computing PCA, MDS, and LR with a scikit.learn-like syntax. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.classes import PCA, MDS, LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_PCA=n_PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `pca.fit(X)` computes the covariance $\\mathbf{C}$ of $\\mathbf{X}$ and internally stores the eigenvectors/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `pca.transform(X)` computes and returns the PCA projection $\\mathbf{T}$. We can also use the eigenvectors from the PCA fit to project new data that was not used to originally construct $\\mathbf{P}_{XT}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_test = pca.transform(X_test)\n",
    "T_train = pca.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_projection(Y_train, T_train, title=\"Projection of Training Data\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_projection(Y_test, T_test, title=\"Projection of Testing Data\", fig=fig, ax=axes[1], **cmaps)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `pca.statistics(X)` outputs the statistics of the projection of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([pca.statistics(X_train), pca.statistics(X_test)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"PCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting Properties with our Low-Dimensional Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's possible to use the projection determined by PCA to regress properties in a manner similar to linear regression. For this, we determine a weight $\\mathbf{W}$ which minimizes\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell = \\lVert\\mathbf{Y}-\\mathbf{TW}\\rVert^2 + \\lambda \\lVert \\mathbf{W} \\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "Analogous to LR, we find this weight to be:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{W} = (\\mathbf{T}^T \\mathbf{T} + \\lambda \\mathbf{I})^{-1} \\mathbf{T}^T \\mathbf{Y}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.matmul(T_train.T, T_train) + best_regularization*np.eye(T_train.shape[1])\n",
    "W = np.linalg.pinv(W)\n",
    "W = np.matmul(W, T_train.T)\n",
    "W = np.matmul(W, Y_train)\n",
    "\n",
    "Y_pca = np.matmul(T_test, W)\n",
    "plot_regression(Y_test[:,0], Y_pca[:,0], **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at how bad that is! As one can imagine, including more components gives a better prediction of the properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "larger_npca = [10, 50, 100, 200]\n",
    "\n",
    "all_stats = [get_stats(y=Y_test, yp=Y_pca)]\n",
    "headers = [r\"$n_{PCA}$\"+\"={}\".format(n_PCA)]\n",
    "\n",
    "for n in larger_npca:\n",
    "    pca2 = PCA(n_PCA=n)\n",
    "    pca2.fit(X_train)\n",
    "\n",
    "    T_train2 = pca2.transform(X_train)\n",
    "    T_test2 = pca2.transform(X_test)\n",
    "\n",
    "    W2 = np.matmul(T_train2.T, T_train2) + best_regularization*np.eye(T_train2.shape[1])\n",
    "    W2 = np.linalg.pinv(W2)\n",
    "    W2 = np.matmul(W2, T_train2.T)\n",
    "    W2 = np.matmul(W2, Y_train)\n",
    "\n",
    "    Y_pca2 = np.matmul(T_test2, W2)\n",
    "    \n",
    "    all_stats.append(get_stats(y=Y_test, yp=Y_pca2))\n",
    "    headers.append(r\"$n_{PCA}$\"+\"={}\".format(n))\n",
    "\n",
    "table_from_dict(all_stats,\n",
    "                 headers = headers, \n",
    "                 title=\"Regression Errors from PCA Projections\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper class is provided to run MDS with a scikit-learn-like syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_MDS=n_MDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `mds.fit(X)` computes the Gram matrix $\\mathbf{K}$ from $\\mathbf{X}$ and internally storess the eigenvectors/values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `mds.transform(X)` computes and returns the MDS projection $\\mathbf{T}$. We also use the eigenvectors from the MDS fit to project new data that was not used to originally construct the MDS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = mds.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=dbl_fig)\n",
    "plot_projection(Y_test, check_mirrors(T, T_ref), fig=fig, ax=ax[0], title=\"Multidimensional Scaling with the Utility Class\", **cmaps)\n",
    "plot_projection(Y_test, T_ref, fig=fig, ax=ax[1], title=\"PCA\", **cmaps)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `mds.statistics(X)` outputs the statistics of the projection of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([mds.statistics(X_train), mds.statistics(X_test)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"MDS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR\n",
    "\n",
    "Unlike PCA, which takes the number of PCA components in the constructor, the linear regression class takes the regularization as a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LR(regularization=best_regularization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `lr.fit(X,Y)` computes the weights $w$ and internally stores them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `lr.transform(X)` computes and return the predicted $\\hat{\\mathbf{Y}}$ values from $\\hat{\\mathbf{Y}}_{LR} = \\mathbf{X}\\mathbf{w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_lr_train = lr.transform(X_train)\n",
    "Y_lr_test = lr.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(Y_train[:,0], Y_lr_train[:,0], title=\"Linear Regression of Training Data\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], Y_lr_test[:,0], title=\"Linear Regression of Testing Data\", fig=fig, ax=axes[1], **cmaps)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `lr.statistics(X,Y)` outputs the statistics of the regression of $\\mathbf{X}$ and $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([lr.statistics(X_train, Y_train), lr.statistics(X_test, Y_test)], \n",
    "                 headers = [\"Training\", \"Testing\"], \n",
    "                 title=\"Linear Regression\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
