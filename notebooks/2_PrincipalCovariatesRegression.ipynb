{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In [the previous notebook](1_LinearMethods.ipynb), we showed linear methods for learning atomic structures. In this notebook, we combine these supervised (LR) and unsupervised (PCA) models into one. \n",
    "\n",
    "As before, for each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. At the end of the notebook, we demonstrate a \"Skcosmo Class\" for the model, which is found in the skcosmo module and contains all necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append(\"../\")\n",
    "from utilities.general import load_variables\n",
    "from utilities.plotting import (\n",
    "    plot_projection,\n",
    "    plot_regression,\n",
    "    check_mirrors,\n",
    "    get_cmaps,\n",
    ")\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from skcosmo.decomposition import PCovR\n",
    "\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use(\"../utilities/kernel_pcovr.mplstyle\")\n",
    "dbl_fig = (2 * plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of Principal Covariates Regression (PCovR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Loss and Similarity Functions\n",
    "\n",
    "Principal Covariates Regression (PCovR) is a mathematical model that combines principal component analysis and linear regression (or more specifically, \n",
    "[principal components regression](https://en.wikipedia.org/wiki/Principal_component_regression)), with a parameter $\\alpha$ that is used to \n",
    "tune the relative weight of each of the two tasks and their corresponding losses [(de Jong, 1992)](https://www.doi.org/10.1016/0169-7439(92)80100-I), \n",
    "[(Vervloet, 2015)](https://www.doi.org/10.18637/jss.v065.i08). \n",
    "\n",
    "\\begin{equation}\n",
    "\\ell =\n",
    "\\alpha {\\left\\lVert \\mathbf{X} - \\mathbf{X}\\mathbf{P}_{XT}\\mathbf{P}_{TX}\\right\\rVert^2}\n",
    " +\n",
    "(1-\\alpha){\\left\\lVert \\mathbf{Y} - \\mathbf{X}\\mathbf{P}_{XT}\\mathbf{P}_{TY}\\right\\rVert^2}.\n",
    "\\end{equation}\n",
    "\n",
    "It is easier to minimize this by looking for a projection $\\tilde{\\mathbf{T}}$ in a latent space if we require $\\tilde{\\mathbf{T}}^T\\tilde{\\mathbf{T}} = \\mathbf{I}$, i.e. for the the column vectors in $\\tilde{\\mathbf{T}}$ to be orthonormal. \n",
    "\n",
    "\\begin{equation}\n",
    "\\ell =\n",
    "\\alpha {\\left\\lVert \\mathbf{X} - \\mathbf{X}\\mathbf{P}_{X\\tilde{T}}\\mathbf{P}_{\\tilde{T}X}\\right\\rVert^2}\n",
    " +\n",
    "(1-\\alpha){\\left\\lVert \\mathbf{Y} - \\mathbf{X}\\mathbf{P}_{X\\tilde{T}}\\mathbf{P}_{\\tilde{T}Y}\\right\\rVert^2}.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\tilde{\\mathbf{T}}$ is the [whitened](https://en.wikipedia.org/wiki/Whitening_transformation) version of our earlier projection $\\mathbf{T}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, $\\mathbf{X}\\mathbf{P}_{X\\tilde{T}} = \\tilde{\\mathbf{T}}$. Due to orthonormality, the definition $\\tilde{\\mathbf{T}}\\mathbf{P}_{\\tilde{T}X} = \\mathbf{X}$ implies that $\\mathbf{P}_{\\tilde{T}X} = \\tilde{\\mathbf{T}}^T\n",
    " \\mathbf{X}$. Similarly, we find  $\\mathbf{P}_{\\tilde{T}Y} = \\tilde{\\mathbf{T}}^T\\mathbf{Y}$, leading to\n",
    "\n",
    "\\begin{equation}\n",
    "    \\ell = \\alpha\\lVert\\mathbf{X} - \\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{X}\\rVert^2 + (1 - \\alpha)\\lVert\\mathbf{Y} - \\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{Y}\\rVert^2.\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: if the features and the properties were not normalized, it would be advisable to do so here, by dividing the first term by ${\\lVert \\mathbf{X} \\rVert^2}$ and the second by ${\\Vert \\mathbf{Y} \\rVert^2}$, to make sure that the two components are compared on equal footings, without introducing a dependence on their absolute magnitude.\n",
    "\n",
    "\n",
    "Just like with [PCA](1_LinearMethods.ipynb), instead of minimizing loss, we maximize the similarity measure, leading to:\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\alpha \\cdot \\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{X}\\mathbf{X}^T + \\left(1-\\alpha\\right)\\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{Y}\\mathbf{Y}^T\\right),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\rho$ is obtained from $\\ell$ by exploiting the invariance of the trace to circular permutations, and dropping constant terms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining X- and Y-Space\n",
    "\n",
    "An important detail to consider is that we are looking for an approximation of the properties based on a reduced-dimensional latent space that depends only on the  $\\mathbf{X}$ features. In order to avoid considering components of the properties that cannot be represented even in the full feature space, we must first project our properties onto $\\mathbf{X}$.\n",
    "\n",
    "For this, we use the least-squares approximation of $\\mathbf{Y}$ as found in [linear regression](1_LinearMethods.ipynb).\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{Y}} = \\mathbf{X}\\mathbf{P}_{XY} = \\mathbf{X}\\left(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}\\right)^{-1}\\mathbf{X}^T \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ is the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 1e-6\n",
    "\n",
    "lr = Ridge(alpha=regularization)\n",
    "lr.fit(X_train, Y_train)\n",
    "Yhat_train = lr.predict(X_train).reshape((-1, Y_train.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When when $\\lambda$ is small, $\\operatorname{Tr}(\\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{Y}\\mathbf{Y}^T) = \\operatorname{Tr}(\\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{\\hat{Y}}\\mathbf{\\hat{Y}}^T)$ we write the similarity function as\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\alpha \\cdot \\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{X}\\mathbf{X}^T + \\left(1-\\alpha\\right)\\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{\\hat{Y}}\\mathbf{\\hat{Y}}^T\\right),\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulations of PCovR\n",
    "\n",
    "In the similarity measure we have the matrix product $\\mathbf{XX}^T$. This matrix becomes too large to handle if $\\mathbf{X}$ has many more rows (samples) than columns (features). Therefore, two formulations of PCovR have been proposed: one for cases where $n_{samples} \\gg n_{features}$, and the other for cases where $n_{features} \\gg n_{samples}$. The former we refer to as **feature space PCovR** (which is analogous to PCA), and we refer to the latter as **sample space PCovR** (which is analogous to MDS).\n",
    "We begin by discussing sample space PCovR."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample-Space PCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute PCovR using the all $n_{samples}$ components of our Sample Space, we must combine the sample-space kernel ($\\mathbf{X}\\mathbf{X}^T$) and the outer product of the regressed properties ($\\hat{\\mathbf{Y}}\\hat{\\mathbf{Y}}^T$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the modified [Gram matrix](https://en.wikipedia.org/wiki/Gramian_matrix)\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{K}} = \\alpha {\\mathbf{X} \\mathbf{X}^T}\n",
    "    + (1 - \\alpha) {\\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}}^T},\n",
    "\\end{equation}\n",
    "\n",
    "Since the trace of the matrix is invariant under cyclic permutation ($\\operatorname{Tr}(\\mathbf{A B C}) = \\operatorname{Tr}(\\mathbf{B C A})  = \\operatorname{Tr}(\\mathbf{C A B})$), we can write \n",
    "$\\rho = \\operatorname{Tr}\\left(\\tilde{\\mathbf{T}}\\tilde{\\mathbf{T}}^T\\mathbf{\\tilde{K}}\\right) = \\operatorname{Tr}\\left(\\tilde{\\mathbf{T}}^T\\mathbf{\\tilde{K}}\\tilde{\\mathbf{T}}\\right)$, that is maximized when $\\tilde{\\mathbf{T}}$ contains the principal eigenvectors of $\\mathbf{\\tilde{K}}$ (i.e., the eigenvectors associated with the largest eigenvalues). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "\n",
    "K_pca = X_train @ X_train.T\n",
    "K_lr = Yhat_train @ Yhat_train.T\n",
    "\n",
    "K = (alpha * K_pca) + (1.0 - alpha) * K_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What follows is analogous to multi-dimensional scaling, with $\\mathbf{\\tilde{K}}$ acting as the Gram matrix, modified to weight structural and property correlations. \n",
    "First, we diagonalize $\\mathbf{\\tilde{K}}=\\mathbf{U_\\tilde{K}}\\mathbf{\\Lambda_\\tilde{K}}\\mathbf{U_\\tilde{K}}^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_Kt, U_Kt = np.linalg.eigh(K)\n",
    "\n",
    "# U_Kt/v_Kt are already sorted, but in *increasing* order, so reverse them\n",
    "U_Kt = np.flip(U_Kt, axis=1)\n",
    "v_Kt = np.flip(v_Kt, axis=0)\n",
    "\n",
    "U_Kt = U_Kt[:,v_Kt>0]\n",
    "v_Kt = v_Kt[v_Kt>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tilde{\\mathbf{T}} = \\mathbf{\\hat{U}_\\tilde{K}}$, where $\\mathbf{\\hat{U}_\\tilde{K}}$ contains the first $n_{PCA}$ components of $\\mathbf{U_\\tilde{K}}$.\n",
    "\n",
    "While it is useful to construct our loss and similarity using the whitened matrix $\\tilde{\\mathbf{T}}$, in doing so we lose information on the relative variance of the different components of the input space. To recover the MDS limit when $\\alpha=1$, we build projections by \"de-whitening\" the principal eigenvectors by multiplying by a factor of $\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{1/2}$, i.e.\n",
    "$\\mathbf{T} = \\tilde{\\mathbf{T}} \\hat{\\mathbf{\\Lambda_\\tilde{K}}}^{1/2} = \\mathbf{X} \\mathbf{P}_{X\\tilde{T}}\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{1/2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = U_Kt[:, :n_PC] @ np.diagflat(np.sqrt(v_Kt[:n_PC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining the Projector $\\mathbf{P}_{XT}$\n",
    "\n",
    "We also derive a projector from $X$ space directly to the latent space in the form of $\\mathbf{T} = \\mathbf{XP}_{XT}$ (and vice versa). For that we need to solve \n",
    "\\begin{equation}\n",
    "\\mathbf{X}\\mathbf{P}_{XT} = \\tilde{\\mathbf{T}} \\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{1/2} =  \\mathbf{\\tilde{K}}\\mathbf{\\hat{U}_\\tilde{K}}\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "where we used the fact that $\\tilde{\\mathbf{T}}$ is built from columns of $\\mathbf{\\hat{U}_{\\tilde{K}}}$.\n",
    "Writing explicitly $\\mathbf{\\tilde{K}}$ and $\\hat{\\mathbf{Y}}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{X}\\mathbf{P}_{XT} &= \\left( \n",
    "\\alpha \\color{red}{\\mathbf{X}} \\mathbf{X}^T\n",
    "+ (1 - \\alpha) \n",
    "\\color{red}{\\mathbf{X}}\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{Y}\n",
    "\\hat{\\mathbf{Y}}^T\n",
    "\\right) \\mathbf{\\hat{U}_\\tilde{K}}\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{-1/2} \\\\\n",
    "&= \\color{red}{\\mathbf{X}} \\left( \n",
    "\\alpha \\mathbf{X}^T\n",
    "+ (1 - \\alpha) \n",
    "\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{Y}\n",
    "\\hat{\\mathbf{Y}}^T\n",
    "\\right) \\mathbf{\\hat{U}_\\tilde{K}} \\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{-1/2}.\n",
    "\\end{align}\n",
    "\n",
    "we obtain the projector from feature to latent space $\\mathbf{P}_{XT}$ explicitly \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{XT} = \\left(\n",
    "\\alpha{\\mathbf{X}^T}\n",
    "+ (1 - \\alpha)\n",
    "\\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^T \\mathbf{Y}\n",
    "\\hat{\\mathbf{Y}}^T\n",
    "\\right)  \\mathbf{\\hat{U}_\\tilde{K}}\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{-1/2}.\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_lr = X_train.T @ X_train + np.eye(X_train.shape[1]) * regularization\n",
    "P_lr = np.linalg.pinv(P_lr)\n",
    "P_lr = (P_lr @ X_train.T @ Y_train).reshape((-1, Y_train.shape[1])) @ Yhat_train.T\n",
    "\n",
    "P_pca = X_train.T\n",
    "\n",
    "P = (alpha * P_pca) + (1.0 - alpha) * P_lr\n",
    "PXT = P @ U_Kt[:, :n_PC] @ np.diag(1 / np.sqrt(v_Kt[:n_PC]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that $\\mathbf{T}\\approx \\mathbf{X}\\mathbf{P}_{XT}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(X_train @ PXT - T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot the projection. Note how the position of points in latent space correlates much better with the properties than they do in PCA latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_pcovr_test = X_test @ PXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "ref = PCA(n_components=n_PC)\n",
    "ref.fit(X_train)\n",
    "t_ref = ref.transform(X_test)\n",
    "\n",
    "plot_projection(\n",
    "    Y_test,\n",
    "    check_mirrors(T_pcovr_test, t_ref),\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    title=r\"PCovR (Sample Space, $\\alpha={}$)\".format(alpha),\n",
    "    **cmaps\n",
    ")\n",
    "plot_projection(Y_test, t_ref, fig=fig, ax=axes[1], title=\"PCA\", **cmaps)\n",
    "\n",
    "fig.suptitle(\n",
    "    r\"These are not the same unless $\\alpha = 1.0.$\",\n",
    "    y=0.0,\n",
    "    fontsize=plt.rcParams[\"font.size\"] + 6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image on the left (PCovR) is the 2D projected that is weighted by a factor of (1-$\\alpha$) towards those axes which best discern the  property vector $\\mathbf{Y}$. Note how the color of the points changes with the position of the points.\n",
    "\n",
    "The image to the right (PCA) is the 2D projection which best preserves the variance of the feature matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Properties\n",
    "Now how do we get the weights to predict the properties? Basically we need to \n",
    "solve the linear regression problem in the projected space, i.e., solve for the weights $\\mathbf{P}_{TY}$ such that:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y_{PCovR}} = \\mathbf{T} \\mathbf{P}_{TY} = \\mathbf{X} \\mathbf{P}_{XT} \\mathbf{P}_{TY}. \n",
    "\\end{equation}\n",
    "\n",
    "We determine the regression parameter $\\mathbf{P}_{TY}$ by applying the usual LR expression using the projections $\\mathbf{T}$.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{TY} = (\\mathbf{T}^T\\mathbf{T})^{-1}\\mathbf{T}^T\\mathbf{Y} = \n",
    " \\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{-1}\\mathbf{T}^T\\mathbf{Y},\n",
    "\\end{equation}\n",
    "\n",
    "Where the factor of $\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{-1}$ arises from the fact that\n",
    "$\\mathbf{T}^T\\mathbf{T}=\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{1/2}\\mathbf{\\hat{U}_\\tilde{K}}^T\\mathbf{\\hat{U}_\\tilde{K}}\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}^{1/2}=\n",
    "\\hat{\\mathbf{\\Lambda}}_\\mathbf{\\tilde{K}}$. Note that $\\mathbf{Y}$ corresponds to the property vector for the train set.\n",
    "\n",
    "**Note:** this differs from the expression in [2015 paper on PCovR](https://www.doi.org/10.18637/jss.v065.i08), which uses the whitened PCA projections $\\tilde{\\mathbf{T}}=\\mathbf{U_\\tilde{K}}$ with variance 1. Here, we construct our PCA projections without this normalization, so we must divide by the eigenvalue matrix to compute regression weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTY = np.diagflat(1 / (v_Kt[:n_PC])) @ T.T @ Y_train\n",
    "Y_pcovr_test = X_test @ PXT @ PTY\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "ref_lr = Ridge(alpha=regularization)\n",
    "ref_lr.fit(X_train, Y_train)\n",
    "yref = lr.predict(X_test)\n",
    "\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    Y_pcovr_test[:, 0],\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    title=r\"PCovR (Sample Space, $\\alpha={}$)\".format(alpha),\n",
    "    **cmaps\n",
    ")\n",
    "plot_regression(Y_test[:, 0], yref[:, 0], fig=fig, ax=axes[1], title=\"LR\", **cmaps)\n",
    "\n",
    "fig.suptitle(\n",
    "    r\"These become more dissimilar as $\\alpha \\to 1.0.$\",\n",
    "    y=0.01,\n",
    "    fontsize=plt.rcParams[\"font.size\"] + 6,\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature-Space PCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Similarity Function in Feature-Space\n",
    "\n",
    "In the case where the number of samples is greater than the number of features ($n_{samples} >> n_{features}$), computing the eigenvectors of $\\mathbf{\\tilde{K}}$ may be undesirable. In this case, we instead compute the eigenvectors of the $n_{features} \\times n_{features}$ matrix, i.e., the feature space. In order to formulate a feature-space version of PCovR we again consider how PCovR maximises the similarity ($\\rho$) between our projection and the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\tilde{\\mathbf{T}}^T\\mathbf{\\tilde{K}}\\tilde{\\mathbf{T}}\\right)\n",
    "\\end{equation}\n",
    "\n",
    "and rewrite it in terms of the decomposition $\\tilde{\\mathbf{T}} = \\mathbf{X}\\mathbf{P}_{X\\tilde{T}}$\n",
    "\n",
    "\\begin{align}\n",
    "\\rho &= \\operatorname{Tr}\\left(\\mathbf{P}_{X\\tilde{T}}^T\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{P}_{X\\tilde{T}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "$\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}$ is a $n_{features}\\times n_{features}$ matrix, so why not solve for $\\mathbf{P}_{X\\tilde{T}}$ through an eigendecomposition of $\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}$?\n",
    "\n",
    "For this trick to work, we need to work with an matrix whose column vectors are orthonormal, but $\\mathbf{P}_{X\\tilde{T}}^T\\mathbf{P}_{X\\tilde{T}} \\neq \\mathbf{I}$! However, \n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{X\\tilde{T}}^T\\mathbf{C}\\mathbf{P}_{X\\tilde{T}} = \\mathbf{P}_{X\\tilde{T}}^T\\mathbf{X}^T\\mathbf{X}\\mathbf{P}_{X\\tilde{T}} = \\tilde{\\mathbf{T}}^T\\tilde{\\mathbf{T}} = \\mathbf{I}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = X_train.T @ X_train\n",
    "\n",
    "PXV = PXT @ np.diagflat(1.0/np.sqrt(v_Kt[:n_PC]))\n",
    "\n",
    "print(np.linalg.norm(PXV.T @ C @ PXV - \n",
    "                     np.eye(PXV.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore $\\mathbf{C}^{1/2}\\mathbf{P}_{X\\tilde{T}}$ contains orthonormal column vectors! \n",
    "\n",
    "We calculate $\\mathbf{C}^{1/2}$ and $\\mathbf{C}^{-1/2}$ using the eigendecomposition of $\\mathbf{C}$, given\n",
    "\n",
    "\\begin{align}\n",
    "& \\mathbf{C}^{1/2} = \\mathbf{U}_C \\mathbf{\\Lambda}^{1/2} \\mathbf{U}_C^T\\\\\n",
    "& \\mathbf{C}^{-1/2} = \\mathbf{U}_C \\mathbf{\\Lambda}^{-1/2} \\mathbf{U}_C^T\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_C, U_C = np.linalg.eigh(C)\n",
    "\n",
    "# U_C/v_C are already sorted, but in *increasing* order, so reverse them\n",
    "U_C = np.flip(U_C, axis=1)\n",
    "v_C = np.flip(v_C, axis=0)\n",
    "\n",
    "U_C = U_C[:,v_C>0]\n",
    "v_C = v_C[v_C>0]\n",
    "\n",
    "Csqrt = U_C @ np.diagflat(np.sqrt(v_C)) @ U_C.T\n",
    "iCsqrt = U_C @ np.diagflat(1.0 / np.sqrt(v_C)) @ U_C.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then write\n",
    "\n",
    "\\begin{align}\n",
    "\\rho &= \\operatorname{Tr}\\left(\\mathbf{P}_{X\\tilde{T}}^T\\mathbf{C}^{1/2}\\mathbf{C}^{-1/2}\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{C}^{-1/2}\\mathbf{C}^{1/2}\\mathbf{P}_{X\\tilde{T}}\\right)\n",
    "\\end{align}\n",
    "\n",
    "and introduce a modified covariance matrix $\\tilde{\\mathbf{C}}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{C}} = \\mathbf{C}^{-1/2}\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{C}^{-1/2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ct = iCsqrt @ X_train.T\n",
    "Ct = Ct @ K @ Ct.T\n",
    "\n",
    "v_Ct, U_Ct = np.linalg.eigh(Ct)\n",
    "U_Ct = np.flip(U_Ct, axis=1)\n",
    "v_Ct = np.flip(v_Ct, axis=0)\n",
    "\n",
    "U_Ct = U_Ct[:,v_Ct>0]\n",
    "v_Ct = v_Ct[v_Ct>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify that $\\mathbf{\\tilde{C}}$ and $\\mathbf{\\tilde{K}}$ have the same eigenvalues, as they're connected to each other by the same relation that links the covariance and the Gram matrix. As such, the blue and red lines in the figure below should be indistinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1)\n",
    "ax.set_yscale(\"log\")\n",
    "ax.plot(v_Kt, marker=\"o\", c=\"b\", label=r\"$\\mathbf{\\tilde{K}}$\")\n",
    "ax.plot(v_Ct, marker=\"o\", c=\"r\", label=r\"$\\mathbf{\\tilde{C}}$\")\n",
    "ax.set_xlabel(\"n\")\n",
    "ax.set_ylabel(\"$\\lambda_n$\")\n",
    "ax.set_title(\n",
    "    r\"Eigenvalues of $\\mathbf{\\tilde{C}}$ and $\\mathbf{\\tilde{K}}$ as a function of n\"\n",
    ")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Projectors\n",
    "\n",
    "The similarity is maximized when the orthonormal column vectors in matrix $\\mathbf{C}^{1/2}\\mathbf{P}_{X\\tilde{T}}$ match the principal eigenvalues of $\\mathbf{\\tilde{C}}$, i.e. $\\mathbf{P}_{X\\tilde{T}} = \\mathbf{C}^{-1/2}\\hat{\\mathbf{U}}_{\\mathbf{\\tilde{C}}}$. Similar to [PCA](1_LinearMethods.ipynb), we require that $\\mathbf{X}_{PcovR} = \\mathbf{\\tilde{T}}\\mathbf{P}_{\\tilde{T}X}$ approximates the portion of the feature space projected to the potential space, i.e., $\\mathbf{X}_{PcovR} \\mathbf{P}_{X\\tilde{T}} = \\mathbf{\\tilde{T}} $, which implies that $\\mathbf{P}_{\\tilde{T}X}\\mathbf{P}_{X\\tilde{T}}=\\mathbf{I}$ and $\\mathbf{P}_{\\tilde{T}X}=\\hat{\\mathbf{U}}_{\\mathbf{\\tilde{C}}}^T \\mathbf{C}^{1/2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PXV = iCsqrt @ U_Ct[:, :n_PC]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general\n",
    "$\\mathbf{P}_{X\\tilde{T}} \\mathbf{P}_{\\tilde{T}X} = \\mathbf{C}^{-1/2}\\hat{\\mathbf{U}}_{\\mathbf{\\tilde{C}}}\\hat{\\mathbf{U}}_{\\mathbf{\\tilde{C}}}^T \\mathbf{C}^{1/2}$ is not a symmetric matrix, and so it is not possible to define an $\\mathbf{P}_{XT}$ such that $\\mathbf{P}_{TX}=\\mathbf{P}_{XT}^{T}$. \n",
    "Consistently with the case of sample-space PCovR, we define \n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\mathbf{P}_{XT} =& \\mathbf{C}^{-1/2}\\hat{\\mathbf{U}}_{\\mathbf{\\tilde{C}}}\\hat{\\mathbf{\\Lambda}}_{\\mathbf{\\tilde{C}}}^{1/2} \\\\\n",
    "\\mathbf{P}_{TX} =& \\hat{\\mathbf{\\Lambda}}_{\\mathbf{\\tilde{C}}}^{-1/2} \\hat{\\mathbf{U}}_{\\mathbf{\\tilde{C}}}^T  \\mathbf{C}^{1/2} \\\\\n",
    "\\mathbf{P}_{TY} =&(\\mathbf{T}^T\\mathbf{T})^{-1}\\mathbf{T}^T\\mathbf{Y} \\\\\n",
    "           =&\\hat{\\mathbf{\\Lambda}}_{\\mathbf{\\tilde{C}}}^{-1/2} \\hat{\\mathbf{U}}_{\\mathbf{\\tilde{C}}}^T  \\mathbf{C}^{-1/2} \\mathbf{X}^T \\mathbf{Y} \\\\\n",
    "\\end{split}\n",
    "\\label{eq:pcovr-projectors}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PXT = PXV @ np.diagflat(np.sqrt(v_Ct[:n_PC]))\n",
    "PTX = np.diagflat(1.0 / np.sqrt(v_Ct[:n_PC])) @ U_Ct[:, :n_PC].T, Csqrt\n",
    "PTY = (\n",
    "    np.diagflat(1.0 / np.sqrt(v_Ct[:n_PC]))\n",
    "    @ U_Ct[:, :n_PC].T\n",
    "    @ iCsqrt\n",
    "    @ X_train.T\n",
    "    @ Y_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting the Data\n",
    "Projecting and regressing in feature-space PCovR proceeds similarly to Sample Space PCovR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_fspcovr_test = X_test @ PXT\n",
    "\n",
    "Y_fspcovr_test = T_fspcovr_test @ PTY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare with the Sample Space PCovR and see that the results are essentially identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "ref = PCovR(mixing=alpha, n_components=2, space=\"sample\")\n",
    "ref.fit(X_train, Y_train)\n",
    "tref = ref.transform(X_test)\n",
    "yref = ref.predict(X_test)\n",
    "xref = ref.inverse_transform(tref)\n",
    "\n",
    "plot_projection(\n",
    "    Y_test,\n",
    "    check_mirrors(T_fspcovr_test, tref),\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    title=\"PCovR (Feature Space)\",\n",
    "    **cmaps\n",
    ")\n",
    "plot_projection(\n",
    "    Y_test, tref, fig=fig, ax=axes[1], title=\"PCovR (Sample Space)\", **cmaps\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    Y_fspcovr_test[:, 0],\n",
    "    fig=fig,\n",
    "    ax=axes[0],\n",
    "    title=r\"PCovR (Feature Space, $\\alpha={}$)\".format(alpha),\n",
    "    cbar=False,\n",
    "    **cmaps\n",
    ")\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    yref[:, 0],\n",
    "    fig=fig,\n",
    "    ax=axes[1],\n",
    "    title=r\"PCovR (Sample Space, $\\alpha={}$)\".format(alpha),\n",
    "    cbar=False,\n",
    "    **cmaps\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find a mirror reflection between the upper-left and upper-right subfigures,  it is because the signs of the eigenvectors are not fixed. Their second principal components $PC_2$ may have the opposite signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCovR Performance\n",
    "\n",
    "To determine the sensitivity of PCovR to the value of $\\alpha$ and the number of PCA components, it is instructive to visualize the dependence of the projections and to the different components of the loss on these parameters. We do so using the skcosmo classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_alphas = 11\n",
    "alphas = np.linspace(0.0, 1.0, n_alphas)\n",
    "components = np.arange(0, 5, 1, dtype=np.int)[2::]\n",
    "n_components = components.size\n",
    "\n",
    "pcovr_calculators = np.array(\n",
    "    [\n",
    "        [PCovR(mixing=a, n_components=c, space=\"feature\") for a in alphas]\n",
    "        for c in components\n",
    "    ]\n",
    ")\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    for adx, a in enumerate(alphas):\n",
    "        pcovr_calculators[cdx][adx].fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of PCovR Projections and Regressions\n",
    "\n",
    "To use PCovR, it's useful to get an intuitive sense for the change in the projections and regressions across $\\alpha$, and see the trade-off. Below we plot both projections and regressions for $n_{\\alpha}$ different values of $\\alpha$.\n",
    "\n",
    "**Note**: Remember that in PCA-like projection, sometimes projections of the same data may be related by mirror reflection. For these projections, this may also occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_plots = int(n_alphas ** 0.5)\n",
    "scale = 3\n",
    "\n",
    "t_ref = pcovr_calculators[0][-3].transform(X_test)\n",
    "y_ref = pcovr_calculators[0][-3].predict(X_test)\n",
    "x_ref = pcovr_calculators[0][-3].inverse_transform(tref)\n",
    "\n",
    "pfig, pax = plt.subplots(\n",
    "    n_plots,\n",
    "    int(np.ceil(n_alphas / n_plots)),\n",
    "    figsize=(\n",
    "        scale * int(np.ceil(n_alphas / n_plots)),\n",
    "        scale * n_plots,\n",
    "    ),\n",
    ")\n",
    "\n",
    "rfig, rax = plt.subplots(\n",
    "    n_plots,\n",
    "    int(np.ceil(n_alphas / n_plots)),\n",
    "    figsize=(\n",
    "        scale * int(np.ceil(n_alphas / n_plots)),\n",
    "        scale * n_plots,\n",
    "    ),\n",
    ")\n",
    "for p, r, pcovr in zip(pax.flatten(), rax.flatten(), pcovr_calculators[0]):\n",
    "\n",
    "    t = pcovr.transform(X_test)\n",
    "    y = pcovr.predict(X_test)\n",
    "    x = pcovr.inverse_transform(t)\n",
    "\n",
    "    plot_projection(\n",
    "        Y_test, check_mirrors(t, t_ref), fig=pfig, ax=p, alpha=1.0, s=20, **cmaps\n",
    "    )\n",
    "\n",
    "    plot_regression(\n",
    "        Y_test[:, 0],\n",
    "        y[:, 0],\n",
    "        fig=pfig,\n",
    "        ax=r,\n",
    "        cbar=False,\n",
    "        vmin=0,\n",
    "        vmax=5,\n",
    "        alpha=1.0,\n",
    "        s=20,\n",
    "        **cmaps,\n",
    "    )\n",
    "\n",
    "    p.set_title(r\"$\\alpha=$\" + str(round(pcovr.mixing, 3)))\n",
    "    r.set_title(r\"$\\alpha=$\" + str(round(pcovr.mixing, 3)))\n",
    "\n",
    "\n",
    "for p, r in zip(pax.flatten()[n_alphas:], rax.flatten()[n_alphas:]):\n",
    "    p.axis(\"off\")\n",
    "    r.axis(\"off\")\n",
    "\n",
    "pfig.subplots_adjust(wspace=0.6, hspace=0.6)\n",
    "pfig.suptitle(r\"Projections across $\\alpha$\")\n",
    "rfig.subplots_adjust(wspace=0.6, hspace=0.6)\n",
    "rfig.suptitle(r\"Regressions across $\\alpha$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of PCovR Loss Terms\n",
    "To get a more quantitative assessment of the behavior of PCovR as a function of $\\alpha$, we plot the errors from the linear regression and PCA terms of the PCovR. The linear regression loss is calculated as the RMSE between our known and predicted properties\n",
    "\n",
    "\\begin{align}\n",
    "\\ell_{regr} = \n",
    " {\\left\\lVert \\mathbf{Y} - \\mathbf{X}\\mathbf{P}_{XT}\\mathbf{P}_{TY}\\right\\rVert^2} \\\\\n",
    "\\end{align}\n",
    "\n",
    "and the variance loss from the reconstruction of the input features\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell_{proj}=\n",
    " {\\left\\lVert \\mathbf{X} - \\mathbf{X}\\mathbf{P}_{XT}\\mathbf{P}_{TX}\\right\\rVert^2}. \\\\\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_pca = np.zeros((n_components, n_alphas))\n",
    "L_lr = np.zeros((n_components, n_alphas))\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    for adx, a in enumerate(alphas):\n",
    "        calculator = pcovr_calculators[cdx][adx]\n",
    "        \n",
    "        # TODO: remove except when score is merged into skcosmo\n",
    "        try:\n",
    "            L_pca[cdx, adx], L_lr[cdx, adx] = calculator.score(X_test, Y_test)\n",
    "        except:\n",
    "            xr = calculator.inverse_transform(calculator.transform(X_test))\n",
    "            yp = calculator.predict(X_test)\n",
    "\n",
    "            L_pca[cdx, adx] = (\n",
    "                np.linalg.norm(X_test - xr) ** 2.0 / np.linalg.norm(X_test) ** 2.0\n",
    "            )\n",
    "            L_lr[cdx, adx] = (\n",
    "                np.linalg.norm(Y_test - yp) ** 2.0 / np.linalg.norm(Y_test) ** 2.0\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, [axsLR, axsPCA] = plt.subplots(1, 2, figsize=dbl_fig, sharex=True)\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    axsLR.plot(alphas, L_lr[cdx, :], marker=\"o\", label=\"{:d} PCs\".format(c))\n",
    "    axsPCA.plot(alphas, L_pca[cdx, :], marker=\"o\", label=\"{:d} PCs\".format(c))\n",
    "\n",
    "axsLR.set_ylabel(r\"$\\ell_{LR}$\")\n",
    "axsLR.set_xlabel(r\"$\\alpha$\")\n",
    "axsPCA.set_ylabel(r\"$\\ell_{PCA}$\")\n",
    "axsPCA.set_xlabel(r\"$\\alpha$\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "axsLR.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the two terms vary in opposite directions (which is obvious, since $\\alpha$ changes the way they contribute to the total loss), choosing a value implies making a tradeoff between LR and PCA performance. The least biased choice is then to look for the value that minimizes the (non-weighted) sum of the PCA and LR terms. \n",
    "\n",
    "This is the optimal choice if one gives equal importance to the two tasks, and is (roughly) equivalent to plotting a $\\lambda$ curve -- i.e. a plot of $\\ell_{LR}$ against $\\ell_{PCA}$ for different values of the mixing $\\alpha$ -- and looking for the elbow of the curve, that gives the value at which the improvement in the accuracy of one of the losses balance the degradation of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=dbl_fig)\n",
    "axsLoss = fig.add_subplot(1, 2, 1)\n",
    "axsSum = fig.add_subplot(1, 2, 2)\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    axsLoss.loglog(L_pca[cdx, :], L_lr[cdx, :], marker=\"o\", label=\"{:d} PCs\".format(c))\n",
    "\n",
    "axsLoss.set_xlabel(r\"$\\ell_{PCA}$\")\n",
    "axsLoss.set_ylabel(r\"$\\ell_{LR}$\")\n",
    "axsLoss.legend()\n",
    "\n",
    "for cdx, c in enumerate(components):\n",
    "    loss_sum = L_lr[cdx, :] + L_pca[cdx, :]\n",
    "    axsSum.semilogy(alphas, loss_sum, marker=\"o\", label=\"{:d} PCs\".format(c))\n",
    "    print(\"Optimal alpha for {:d} PCs = {:.2f}\".format(c, alphas[np.argmin(loss_sum)]))\n",
    "\n",
    "axsSum.set_xlabel(r\"$\\alpha$\")\n",
    "axsSum.set_ylabel(r\"$\\ell_{LR} + \\ell_{PCA}$\")\n",
    "\n",
    "fig.subplots_adjust(wspace=0.5, hspace=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Kernel Methods\n",
    "\n",
    "Continue on to the [next notebook](3_KernelMethods.ipynb)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation in `skcosmo`\n",
    "\n",
    "Classes from the skcosmo module enable computing PCovR with a scikit.learn-like syntax. \n",
    "\n",
    "The PCovR class takes a parameter `space` to designate whether projectors should be computed in structure or feature space. If the parameter is not supplied, the class detects the shape of the input data when `pcovr.fit(X,Y)` is called and chooses the most efficient method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skcosmo.decomposition import PCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***To avoid confusion with scikit-learn naming conventions, where `alpha` is a regularization parameter, the keyword `mixing` is used to specify the PCovR $\\alpha$.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "pcovr = PCovR(mixing=alpha, n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcovr.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pcovr.transform(X)` returns the projection $\\mathbf{T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pcovr.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pcovr.predict(X)` returns the regressed properties $\\mathbf{Y}_p$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pcovr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pcovr.predict(T)` returns the reconstructed input data $\\mathbf{X}_r$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = pcovr.inverse_transform(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=dbl_fig)\n",
    "plot_projection(\n",
    "    Y_test, t, title=r\"PCovR ($\\alpha$={})\".format(alpha), fig=fig, ax=ax[0], **cmaps\n",
    ")\n",
    "plot_regression(\n",
    "    Y_test[:, 0],\n",
    "    y[:, 0],\n",
    "    title=r\"PCovR ($\\alpha$={})\".format(alpha),\n",
    "    fig=fig,\n",
    "    ax=ax[1],\n",
    "    **cmaps\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.934px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
