{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook, we demonstrate how to adapt the kernel methods shown in the [previous notebook](3_KernelMethods.ipynb) to use sparse kernels.\n",
    "\n",
    "As for the previous notebooks, for each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. At the end of the notebook, we employ a \"Utility Class\" for the model, which is found in the utilities folder and contains all necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append('../')\n",
    "from utilities.general import FPS, load_variables, sorted_eig, get_stats\n",
    "from utilities.plotting import (\n",
    "    plot_projection, plot_regression, check_mirrors, get_cmaps, table_from_dict\n",
    ")\n",
    "from utilities.kernels import linear_kernel, gaussian_kernel, center_kernel\n",
    "from utilities.classes import KPCA, KRR, SparseKPCA, SparseKRR\n",
    "from utilities.kpcovr import KPCovR, SparseKPCovR\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use('../utilities/kernel_pcovr.mplstyle')\n",
    "dbl_fig=(2*plt.rcParams['figure.figsize'][0], plt.rcParams['figure.figsize'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables()\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing a Sparse Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change this cell to change the kernel function throughout\n",
    "\n",
    "kernel_func = gaussian_kernel\n",
    "kernel_type = 'gaussian'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Nystr&ouml;m Approximation\n",
    "\n",
    "In sparse kernel methods, an approximate kernel is used in place of the full kernel. This approximate kernel is typically constructed according to the [Nystr&ouml;m approximation](https://en.wikipedia.org/wiki/Low-rank_matrix_approximations#Nystr%C3%B6m_approximation) [(Williams 2001)](http://papers.nips.cc/paper/1866-using-the-nystrom-method-to-speed-up-kernel-machines.pdf),\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{K} \\approx \\mathbf{\\tilde{K}}_{NN} = \\mathbf{K}_{NM} \\mathbf{K}_{MM}^{-1} \\mathbf{K}_{NM}^T,\n",
    "\\end{equation}\n",
    "\n",
    "Here, $M$ represents a subset of the $N$ total rows/columns of the kernel matrix, i.e. the kernel between a small **active set** than is selected with subsampling method, like farthest point sampling (FPS) [(Eldar 1997)](https://doi.org/10.1109/83.623193), or a CUR decomposition [(Imbalzano2018)](https://doi.org/10.1063/1.5024611), that is discussed in the [next notebook](5_CUR.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_active = 20\n",
    "\n",
    "fps_idxs, _ = FPS(X_train, n_active)\n",
    "Xsparse = X_train[fps_idxs, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, $\\mathbf{K}_{NM}$ is the kernel matrix between input data $\\mathbf{X}$ and $\\mathbf{X_{sparse}}$, a version of $\\mathbf{X}$ containing only the active set. $\\mathbf{K}_{MM}$ is the matrix containing the kernel evaluated between the active set samples.\n",
    "\n",
    "We center our kernels with $\\mathbf{K}_{MM}$ as our reference, i.e. for any kernel $\\mathbf{K}_{NM}$, the centered kernel is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{K}}_{NM} = \n",
    "\\mathbf{K}_{NM} -  \\mathbf{1}_{NM} \\mathbf{K}_{MM} -  \\mathbf{K}_{NM}\\mathbf{1}_{MM}\n",
    "+ \\mathbf{1}_{NM} \\mathbf{K}_{MM} \\mathbf{1}_{MM}.\n",
    "\\end{equation}\n",
    "\n",
    "analogous to what discussed in [the previous notebook](3_KernelMethods.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Kmm_raw = kernel_func(Xsparse, Xsparse)\n",
    "Kmm = center_kernel(Kmm_raw) \n",
    "\n",
    "Knm_train = kernel_func(X_train, Xsparse)\n",
    "Knm_train = center_kernel(Knm_train, reference=Kmm_raw)\n",
    "\n",
    "Knm_test = kernel_func(X_test, Xsparse)\n",
    "Knm_test = center_kernel(Knm_test, reference=Kmm_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Explicit RKHS\n",
    "\n",
    "Sometimes, it might be more convenient to explicitly write out the projection of the training points\n",
    "on the [RKHS](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space) defined by the active set.\n",
    "This is essentially a KPCA built for the active set, that is not truncated to a few eigenvectors,\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\Phi}_{NM} = \\mathbf{K}_{NM} \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}.\n",
    "\\end{equation}\n",
    "\n",
    "Using this definition it is easy to derive the Nystr√∂m approximation: \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{K}}_{NN} = \\mathbf{\\Phi}_{NM} \\mathbf{\\Phi}_{NM}^T = \n",
    "\\mathbf{K}_{NM}  \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1}  \\mathbf{U}_{MM}^T \\mathbf{K}_{NM}^T\n",
    "= \\mathbf{K}_{NM} \\mathbf{K}_{MM}^{-1} \\mathbf{K}_{NM}^T.\n",
    "\\end{equation}\n",
    "\n",
    "It might be wise to discard some of the smaller eigenvalues. For instance, if it has been centered, $\\mathbf{K}_{MM}$ has one _exactly_ zero eigenvalue, and we should take it out of the projection. [(Honeine 2014)](https://arxiv.org/pdf/1407.2904.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmm, Umm = sorted_eig(Kmm, thresh=1e-12)\n",
    "\n",
    "Phi = np.matmul(Knm_train, Umm[:,:n_active-1])\n",
    "Phi = np.matmul(Phi, np.diagflat(1.0/np.sqrt(vmm[0:n_active-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Centering the RKHS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $\\mathbf{\\Phi}_{NM}$ is also centered relative to the active set, $\\mathbf{X}$ might well be centered otherwise. We must re-center according to the distribution of $\\mathbf{X}$.  \n",
    "We introduce the centered version, $\\mathbf{\\tilde{\\Phi}}_{NM} = \\mathbf{\\Phi}_{NM} - \\mathbf{\\bar{\\Phi}}_{M}$. If you represent each element of $\\mathbf{\\Phi}$ in its summation form\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\Phi}_{nm} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(K_{nm'} U_{m'm}\\right), \n",
    "\\end{equation}\n",
    "\n",
    "then the column means are given by\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\bar{\\Phi}}_{m} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(\\left(\\frac{1}{N}\\sum_n^N K_{nm'}\\right)U_{m'm} \\right), \n",
    "\\end{equation}\n",
    "\n",
    "so the centered feature matrix is computed by $\\mathbf{K}_{NM}$, centered by the column means of the kernel, as denoted by $\\mathbf{\\bar{K}}_M$.\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{\\Phi}}_{NM} =  \\left(\\mathbf{K}_{NM} -\\mathbf{\\bar{K}}_M\\right) \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}.\n",
    "\\end{equation}\n",
    "\n",
    "It is best to keep the column mean $\\mathbf{\\bar{K}}_M$ separate, because it has to be used also when performing out-of-sample embedding, where $\\mathbf{K}_{NM}$ would corresponds to the test set kernel. For consistency, $\\mathbf{\\bar{K}}_M$ must always be the kernel mean associated with the train set.\n",
    "\n",
    "Alternatively, one can store $\\mathbf{\\bar{\\Phi}}_{M}$ and use it for centering.\n",
    "\n",
    "**Note**: in the following we often use $\\mathbf{\\Phi}_{NM}$ and $\\mathbf{\\tilde{\\Phi}}_{NM}$ without the subscripts to indicate the train set features approximated in the active RKHS. \n",
    "\n",
    "<!---\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{\\Phi}}_{nm} = \\frac{1}{\\sqrt{\\Lambda_{mm}}}\\sum_{m'}^M \\left(\\left(K_{nm'} - \\frac{1}{N}\\sum_{n'}^N K_{n'm'}\\right)U_{m'm}\\right)\n",
    "\\end{equation}\n",
    "\\end{comment}\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "barKm = np.mean(Kmm, axis=0)\n",
    "\n",
    "barPhi = np.mean(Phi, axis=0)\n",
    "Phi -= barPhi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparse kernel principal component analysis (sKPCA) is formulated in the same way as standard KPCA, with the exception that an approximate kernel matrix is used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{\\tilde{\\Phi}}$ is the feature matrix for the train points in the RKHS defined by the $M$ active set. Sparse KPCA can be understood (and derived) as PCA in the active set RKHS, by computing and diagonalising the covariance matrix built from $\\mathbf{\\Phi}_{NM}$. The covariance should be computed using *centered* kernel features, as discussed above\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} = \\mathbf{\\tilde{\\Phi}}^T \\mathbf{\\tilde{\\Phi}} = \\mathbf{U}_C \\mathbf{\\Lambda}_C \\mathbf{U}_C^T.\n",
    "\\end{equation}\n",
    "\n",
    "Note that - as usual - one could also compute the RKHS covariance without explicitly diagonalising $\\mathbf{K}_{MM}$, as \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C} = \n",
    " \\left(\\mathbf{K}_{NM} -\\mathbf{\\bar{K}}_M\\right) \\mathbf{K}_{MM}^{-1}  \\left(\\mathbf{K}_{NM} -\\mathbf{\\bar{K}}_M\\right)^T,\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{\\bar{K}}_M$ indicates the centering vector as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(Phi.T, Phi)\n",
    "\n",
    "v_C, U_C = sorted_eig(C, thresh=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting the Sparse KPCA\n",
    "Projecting into latent space, we get\n",
    "\n",
    "\\begin{align}\n",
    "    \\mathbf{T} &= \\mathbf{\\tilde{\\Phi}} \\hat{\\mathbf{U}}_C \\\\\n",
    "    &= \\left(\\mathbf{K}_{NM}- \\bar{\\mathbf{K}}_M\\right)\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\hat{\\mathbf{U}}_C \\\\\n",
    "    &= \\mathbf{K}_{NM}\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\hat{\\mathbf{U}}_C -\\bar{\\mathbf{\\Phi}}\\hat{\\mathbf{U}}_C\\\\\n",
    "    &= \\mathbf{K}_{NM} \\mathbf{P}_{KT} - \\mathbf{\\bar{T}}\n",
    "\\end{align}\n",
    "\n",
    "where our sKPCA projector from kernel space $\\mathbf{P}_{KT} = \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{\\hat{U}}_C$, where $\\mathbf{\\hat{U}}_C$ contains the first $n_{PCA}$ eigenvectors of $\\mathbf{C}$. $\\mathbf{\\bar{T}} = \\bar{\\mathbf{\\Phi}}\\hat{\\mathbf{U}}_C$ centers in the latent space, and is computed and stored for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = np.matmul(Umm[:,:n_active-1],\\\n",
    "                    np.diagflat(1.0/np.sqrt(vmm[0:n_active-1])))\n",
    "PKT = np.matmul(PKT, U_C[:, :n_PCA])\n",
    "barT = np.matmul(barPhi, U_C[:, :n_PCA])\n",
    "\n",
    "T_train = np.matmul(Knm_train, PKT) - barT\n",
    "T_test = np.matmul(Knm_test, PKT) - barT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "ref_kpca = KPCA(n_KPCA=n_PCA, kernel_type=kernel_type)\n",
    "ref_kpca.fit(X_train, K=K_train)\n",
    "xref = ref_kpca.transform(X_test, K=K_test)\n",
    "\n",
    "plot_projection(Y_test, check_mirrors(T_test, xref), fig=fig, ax=axes[0], \\\n",
    "                 title=\"Sparse KPCA on {} Environments\".format(Kmm.shape[0]),\n",
    "                  **cmaps\n",
    "         )\n",
    "plot_projection(Y_test,  xref, fig=fig, ax=axes[1], \\\n",
    "                title=\"KPCA on {} Environments\".format(X_train.shape[0]),\n",
    "               **cmaps\n",
    "         )\n",
    "\n",
    "fig.subplots_adjust(wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reconstruct $\\mathbf{X}$ using $\\mathbf{P}_{TX} = \\mathbf{\\Lambda}^{-1}\\mathbf{T}^T\\mathbf{X}$, as in KPCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTX = np.matmul(np.diagflat(1.0/(v_C[:n_PCA])),np.matmul(T_train.T, X_train))\n",
    "\n",
    "Xr_test = np.matmul(T_test, PTX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "The same loss functions are used as in KPCA, so we can compare the loss with that of KPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_approx_train = np.matmul(T_train,T_train.T)\n",
    "\n",
    "K_test_test = kernel_func(X_test, X_test)\n",
    "K_test_test = center_kernel(K_test_test)\n",
    "K_approx_test = np.matmul(T_test,T_test.T)\n",
    "\n",
    "table_from_dict([ref_kpca.statistics(X_test, Y_test, K=K_test),\n",
    "                 get_stats(x=X_test, \n",
    "                           xr=Xr_test,\n",
    "                           y=Y_test, \n",
    "                           t=T_test, \n",
    "                           k=K_test_test, \n",
    "                           kapprox=K_approx_test)], \n",
    "                 headers = [\"KPCA\", \"sKPCA\"], \n",
    "                 title=\"sKPCA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KRR\n",
    "\n",
    "## Sparse KRR Weights\n",
    "Let's see how sparsity works out in the case of regression. \n",
    "\n",
    "If we now build a (regularized) linear regression in the RKHS we get the loss\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell = \\lVert \\mathbf{Y} - \\mathbf{\\Phi}\\mathbf{P}_{\\Phi Y} \\rVert^2 + \n",
    "\\lambda \\lVert\\mathbf{P}_{\\Phi Y} \\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "This is solved by \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\Phi Y} = \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}+ \\lambda \\mathbf{I}\\right)^{-1} \\mathbf{\\Phi}^T \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "or, by writing the last $\\mathbf{\\Phi}^T$ in terms of the kernel:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\Phi Y} = \\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}+ \\lambda \\mathbf{I}\\right)^{-1} \\mathbf{\\Lambda}_{MM}^{-1/2} \\mathbf{U}_{MM}^T \\mathbf{K}_{NM}^T  \\mathbf{Y}\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: Since (kernel) ridge regression is often performed without centering the kernel, we use the uncentered feature matrix $\\mathbf{\\Phi}$ instead of $\\mathbf{\\tilde{\\Phi}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization = 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from after we've computed our sparse kernels and recompute $\\mathbf{\\Phi}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vmm, Umm = sorted_eig(Kmm, thresh=0)\n",
    "\n",
    "Phi_raw = np.matmul(Knm_train, Umm[:,:n_active-1])\n",
    "Phi_raw = np.matmul(Phi_raw, np.diagflat(1.0/np.sqrt(vmm[0:n_active-1])))\n",
    "\n",
    "barKM = np.mean(Knm_train, axis=0)\n",
    "Phi = Knm_train\n",
    "Phi = np.matmul(Knm_train, Umm[:,:n_active-1])\n",
    "Phi = np.matmul(Phi, np.diagflat(1.0/np.sqrt(vmm[0:n_active-1])))\n",
    "barPhi = np.mean(Phi, axis=0)\n",
    "Phi -= barPhi\n",
    "\n",
    "PPY = np.matmul(Phi.T, Phi)\n",
    "PPY = PPY + regularization*np.eye(Phi.shape[1])\n",
    "PPY = np.linalg.pinv(PPY)\n",
    "PPY = np.matmul(PPY, Phi.T)\n",
    "PPY = np.matmul(PPY, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Often Cheaper, More Elegant Route\n",
    "\n",
    "We cast this expression into the more commonly used form by a series of simple manipulations that remove the need for diagonalizing $K_{MM}$ and computing $\\mathbf{\\Phi}$. First, we redefine the weights so that \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\Phi}\\mathbf{P}_{\\Phi Y} = \n",
    "\\mathbf{K}_{NM} \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2} \\mathbf{P}_{\\Phi Y} = \n",
    "\\mathbf{K}_{NM} \\tilde{\\mathbf{P}_{K Y}}.\n",
    "\\end{equation}\n",
    "\n",
    "Then,\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{P}_{K Y}}  &= \n",
    "\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{P}_{\\Phi Y} \\\\\n",
    "& = \\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\left(\\mathbf{\\Phi}^T \\mathbf{\\Phi}+ \\lambda \\mathbf{I}_M\\right)^{-1} \n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}  \\mathbf{U}_{MM}^T \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}\\\\\n",
    "& = \n",
    "\\left(\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{1/2}\\mathbf{\\Phi}^T \\mathbf{\\Phi}\\mathbf{\\Lambda}_{MM}^{1/2}\\mathbf{U}_{MM}^T+ \\lambda \\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}\\mathbf{U}_{MM}^T\\right)^{-1} \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}.\n",
    "\\end{align}\n",
    "\n",
    "Now, by noting that \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{U}_{MM} \\mathbf{\\Lambda}_{MM}^{1/2} \n",
    "\\mathbf{\\Phi}^T \\mathbf{\\Phi}\n",
    "\\mathbf{\\Lambda}_{MM}^{1/2}  \\mathbf{U}_{MM}^T  = \n",
    "\\mathbf{K}_{NM}^T \\mathbf{K}_{NM},\n",
    "\\end{equation}\n",
    "\n",
    "we see that the sparse KRR model weights is computed by\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{P}_{K Y}}  = \n",
    "\\left(\\mathbf{K}_{NM}^T \\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1} \n",
    "\\mathbf{K}_{NM}^T \\mathbf{Y}.\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "PKY = np.matmul(Knm_train.T, Knm_train)\n",
    "PKY = PKY + regularization*Kmm\n",
    "PKY = np.linalg.pinv(PKY)\n",
    "PKY = np.matmul(PKY, Knm_train.T)\n",
    "PKY = np.matmul(PKY, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, this trick provides a (in some cases considerable) speed-up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_skrr_train = np.matmul(Knm_train, PKY)\n",
    "Y_skrr_test = np.matmul(Knm_test, PKY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare our results with those from KRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "ref_krr = KRR(regularization=regularization, kernel_type=kernel_type)\n",
    "ref_krr.fit(X=X_train, Y=Y_train, K=K_train)\n",
    "Y_krr = ref_krr.transform(X=X_test, K=K_test)\n",
    "\n",
    "plot_regression(Y_test[:,0], Y_krr[:,0], title=\"KRR\", fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], Y_skrr_test[:,0], title=\"Sparse KRR on {} Environments\".format(n_active), fig=fig, ax=axes[1], **cmaps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error and Loss\n",
    "\n",
    "Here our loss function is:\n",
    "\n",
    "\\begin{equation}\n",
    "\\ell = \\left\\lVert \\mathbf{Y} - \\mathbf{K}_{NM}\\mathbf{P}_{KY}\\right\\rVert^2\n",
    "\\end{equation}\n",
    "\n",
    "which we compare with KRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([ref_krr.statistics(X_test, Y_test, K=K_test),\n",
    "                 get_stats(x=X_test, \n",
    "                           y=Y_test, \n",
    "                           yp = Y_skrr_test,\n",
    "                          )], \n",
    "                 headers = [\"KRR\", \"sKRR\"], \n",
    "                 title=\"Ridge Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse KPCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Sparse KPCovR, instead of using the Nystr&ouml;m approximation as in previous sparse methods, we formulate sparse KPCovR from KPCovR in a similar manner to how we derived feature space PCovR from structure space PCovR in the [PCovR Notebook](2_PrincipalCovariatesRegression.ipynb).\n",
    "\n",
    "## A (Very) Quick Recap of Sample and Feature Space PCovR\n",
    "In PCovR, we maximize the similarity\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\mathbf{V}^T\\mathbf{\\tilde{K}}\\mathbf{V}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "by taking as our whitened projection $\\mathbf{V} = \\mathbf{XP}_{XV}$ the eigenvectors corresponding to the $n_{PCA}$ largest eigenvalues of\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\tilde{K}} = \\alpha {\\mathbf{X} \\mathbf{X}^T}\n",
    "    + (1 - \\alpha) {\\hat{\\mathbf{Y}} \\hat{\\mathbf{Y}}^T},\n",
    "\\end{equation}\n",
    "\n",
    "which combines correlations between the samples in feature and property space. \n",
    "\n",
    "If the number of features is less than the number of samples, we can equivalently rewrite our similarity function as\n",
    "\n",
    "\\begin{align}\n",
    "\\rho &= \\operatorname{Tr}\\left(\\mathbf{P}_{XV}^T\\mathbf{C}^{1/2}\\mathbf{C}^{-1/2}\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{C}^{1/2}\\mathbf{C}^{-1/2}\\mathbf{P}_{XV}\\right)\n",
    "\\end{align}\n",
    "\n",
    "and diagonalize a modified covariance\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{C}} = \\mathbf{C}^{-1/2}\\mathbf{X}^T\\mathbf{\\tilde{K}}\\mathbf{X}\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X}$ to avoid diagonalizing the $n_{samples} \\times n_{samples}$ matrix $\\tilde{\\mathbf{K}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we just do feature-space PCovR in the RKHS\n",
    "\n",
    "In KPCovR, we maximize the similarity\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho = \\operatorname{Tr}\\left(\\mathbf{V}^T\\mathbf{\\tilde{K}}\\mathbf{V}\\right),\n",
    "\\end{equation}\n",
    "\n",
    "however here $\\mathbf{V} = \\mathbf{KP}_{KT}$.\n",
    "We compute the projection in feature space by maximizing:\n",
    "\n",
    "\\begin{align}\n",
    "\\rho = \\operatorname{Tr}\\left(\\mathbf{P}_{KT}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}_{KT}\\right)\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{K} = \\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T$.\n",
    "It would make sense to use $\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}$ as our sparse \"kernel\", but we must insert an identity to ensure that its eigenvectors are orthogonal. We use the covariance, defined as $\\mathbf{C} = \\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{\\Phi}}$, giving:\n",
    "\n",
    "\\begin{align}\n",
    "\\rho=\\operatorname{Tr}\\left(\\mathbf{P}_{KT}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2}\n",
    "\\mathbf{C}^{1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}_{KT}\\right)\n",
    "\\end{align}\n",
    "\n",
    "which ensures orthogonality, as \n",
    "\n",
    "\\begin{align}\n",
    "\\left(\\mathbf{P}_{KT}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{1/2}\n",
    "\\mathbf{C}^{1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}\\right)\n",
    "&=\\left(\\mathbf{P}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{P}\\right)\\\\\n",
    "&=\\left(\\mathbf{P}^T\\mathbf{K}^T\\mathbf{K}\\mathbf{P}\\right)\\\\\n",
    "&=\\left(\\mathbf{V}^T\\mathbf{V}\\right)\\\\\n",
    "&=\\mathbf{I}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.dot(Phi.T, Phi)\n",
    "\n",
    "v_C, U_C = sorted_eig(C, thresh=0)\n",
    "U_C = U_C[:, v_C>0]\n",
    "v_C = v_C[v_C>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In analogy with to feature-space PCovR, we define\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\tilde{K}}\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "which evaluates to\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\\mathbf{\\tilde{\\Phi}}^T\\mathbf{\\hat{Y}}\n",
    "\\mathbf{\\hat{Y}}^T\\mathbf{\\tilde{\\Phi}}\\mathbf{C}^{-1/2},\n",
    "\\end{equation}\n",
    "\n",
    "**Note**: for consistency, we cannot substitute $\\mathbf{\\hat{Y}}$ with $\\mathbf{Y}_{sKRR} = \\mathbf{K}_{NM}\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)\\mathbf{K}_{NM}^T \\mathbf{Y}$ unless we use a centered feature matrix for sparse KRR. Given that we need to compute the eigenvalue decomposition of $\\mathbf{K}_{MM}$ to orthogonalize the modified covariance, we can instead use the linear regression solution computed directly in active points RKHS: $\\mathbf{Y}_{sKRR} = \\mathbf{\\tilde{\\Phi}}\\left(\\mathbf{C}+ \\lambda \\mathbf{I}\\right)^{-1}\\mathbf{\\tilde{\\Phi}}^T \\mathbf{Y}$.\n",
    "\n",
    "<!---\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{U}_{MM}^T\n",
    "\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T \n",
    "\\mathbf{Y}\n",
    "\\mathbf{Y}^T\n",
    "\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}+ \\lambda \\mathbf{K}_{MM}\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\n",
    "\\mathbf{U}_{MM}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{C}} = \\alpha \\frac{\\mathbf{C}} {\\operatorname{Tr}(\\mathbf{C})/N} + (1 - \\alpha) \\mathbf{C}^{-1/2}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{U}_{MM}^T\n",
    "\\left(\\mathbf{I}_{M}+ \\lambda \n",
    "\\mathbf{K}_{MM}\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\\right)^{-1}\n",
    "\\right)^{-1}\n",
    "\\mathbf{K}_{NM}^T \n",
    "\\mathbf{Y}\n",
    "\\mathbf{Y}^T\n",
    "\\mathbf{K}_{NM}\n",
    "\\left(\\mathbf{I}_{M}+ \\lambda \n",
    "\\mathbf{K}_{MM}\\left(\\mathbf{K}_{NM}^T\\mathbf{K}_{NM}\\right)^{-1}\n",
    "\\right)^{-1}\n",
    "\\mathbf{U}_{MM}\n",
    "\\mathbf{\\Lambda}_{MM}^{-1/2}\n",
    "\\mathbf{C}^{-1/2}\n",
    "\\end{equation}\n",
    "--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "regularization=1e-6\n",
    "\n",
    "Csqrt = np.matmul(np.matmul(U_C, np.diagflat(np.sqrt(v_C))), U_C.T)\n",
    "iCsqrt = np.matmul(np.matmul(U_C, np.diagflat(1.0/np.sqrt(v_C))), U_C.T)\n",
    "\n",
    "C_pca = C / (np.trace(C)/C.shape[0])\n",
    "\n",
    "C_lr = np.linalg.pinv(C + regularization*np.eye(C.shape[0]))\n",
    "C_lr = np.matmul(Phi, C_lr)\n",
    "C_lr = np.matmul(Phi.T, C_lr)\n",
    "C_lr = np.matmul(iCsqrt, C_lr)\n",
    "C_lr = np.matmul(C_lr, Phi.T)\n",
    "C_lr = np.matmul(C_lr, Y_train.reshape(-1,Y_train.shape[-1]))\n",
    "C_lr = np.matmul(C_lr, C_lr.T)\n",
    "\n",
    "Ct = alpha*C_pca + (1-alpha)*C_lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then find the eigendecomposition of \n",
    "$\\mathbf{\\tilde{C}}=\n",
    "\\mathbf{U}_\\mathbf{\\tilde{C}}\\mathbf{\\Lambda}_\\mathbf{\\tilde{C}}\\mathbf{U}_\\mathbf{\\tilde{C}}^T$  and \n",
    "solve for $\\mathbf{P}_{\\tilde{\\Phi} T}$ (again analogous to feature-space PCovR, swapping $\\mathbf{\\Phi}$ for $\\mathbf{X}$): \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{\\tilde{\\Phi} T}=\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} \n",
    "\\end{equation}\n",
    "\n",
    "where the $\\hat{\\cdot}$ decoration denotes a truncation to $n_{PCA}$ components, as usual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_Ct, U_Ct = sorted_eig(Ct, thresh=0)\n",
    "\n",
    "PPT = np.matmul(iCsqrt, U_Ct[:, :n_PCA])\n",
    "PPT = np.matmul(PPT, np.diag(np.sqrt(v_Ct[:n_PCA])))\n",
    "v_Ct.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting into Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our projection in feature space takes the form:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{\\tilde{\\Phi}}_{NM}\\mathbf{P}_{\\tilde{\\Phi} T}\n",
    "\\end{equation}\n",
    "\n",
    "If we want to project using a kernel rather than a feature space vector, this becomes:\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{T} &=  \\left(\\mathbf{K}_{NM}-\\mathbf{\\bar{K}}_M\\right)\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{P}_{\\tilde{\\Phi} T} \\\\\n",
    "&=\\mathbf{K}_{NM}\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} - \\mathbf{\\bar{\\Phi}}\\mathbf{P}_{\\tilde{\\Phi} T} \\\\\n",
    "&= \\mathbf{K}_{NM}\\mathbf{P}_{KT} -  \\mathbf{\\bar{T}}\n",
    "\\end{align}\n",
    "\n",
    "where $\\mathbf{P}_{K T} = \\mathbf{P}_{K\\Phi} \\mathbf{P}_{\\Phi T} = \n",
    "\\mathbf{U}_{MM}\\mathbf{\\Lambda}_{MM}^{-1/2}\\mathbf{C}^{-1/2}\\mathbf{\\hat{U}}_\\mathbf{\\tilde{C}}\\mathbf{\\hat{\\Lambda}}_\\mathbf{\\tilde{C}}^{1/2} $."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PKT = np.matmul(Umm[:, :n_active-1], np.diagflat(1/np.sqrt(vmm[:n_active-1])))\n",
    "PKT = np.matmul(PKT, PPT)\n",
    "barT = np.matmul(barPhi, PPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T =  np.matmul(Knm_train, PKT) - barT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_skpcovr_test = np.matmul(Knm_test, PKT) - barT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We again compare to the non-sparse kernel version, giving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "ref = KPCovR(alpha=alpha, n_PCA=2, kernel_type=kernel_type)\n",
    "ref.fit(X_train, Y_train)\n",
    "xref, yref, r = ref.transform(X_test)\n",
    "\n",
    "plot_projection(Y_test, check_mirrors(T_skpcovr_test, xref), fig=fig, ax=axes[0], title = \"Sparse KPCovR\", **cmaps)\n",
    "plot_projection(Y_test, xref, fig=fig, ax=axes[1], title = \"KPCovR\", **cmaps)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting the Properties\n",
    "\n",
    "Property prediction takes the exact same form as in KPCovR, except with $\\mathbf{T}$ supplied by our sparse construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PTY = np.matmul(T.T, T)\n",
    "PTY = np.linalg.pinv(PTY)\n",
    "PTY = np.matmul(PTY, T.T)\n",
    "PTY = np.matmul(PTY, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ypred = np.matmul(Knm_test, PKT)\n",
    "Ypred = np.matmul(Ypred, PTY)\n",
    "\n",
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_regression(Y_test[:,0], Ypred[:,0], fig=fig, ax=axes[0], title = f\"Sparse KPCovR with {n_active} Environments\", **cmaps)\n",
    "plot_regression(Y_test[:,0], yref[:,0], fig=fig, ax=axes[1], title = \"KPCovR\", **cmaps)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: CUR Decomposition and Feature Selection\n",
    "\n",
    "Continue on to the [next notebook](5_CUR.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Utility Classes\n",
    "\n",
    "Classes from the utility module enable computing SparseKPCA, SparseKRR, and SparseKPCovR with a scikit.learn-like syntax. `SparseKPCovR` is located in `utilities/kpcovr.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.classes import SparseKPCA, SparseKRR\n",
    "from utilities.kpcovr import SparseKPCovR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: In all sparse kernel classes, the functions `fit`, `transform`, and `statistics` either computes the designated kernels for the supplied $\\mathbf{X}$ data or uses the provided precomputed kernels. The kernel are **always** a keyword argument (e.g. `model.fit(Kmm=Kmm, Knm=Knm)`), and the first argument, positionally, is $\\mathbf{X}$.\n",
    "\n",
    "In each demonstration, we show the function signature using X, but we also supply our precomputed kernel for computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse KPCA with Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca = SparseKPCA(n_KPCA=2, n_active=n_active, kernel_type=kernel_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skpca.fit(X)` computes the eigendecomposition and internally stores it for projection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skpca.fit(X_train, Kmm=Kmm, Knm=Knm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skpca.transform(X)` computes the projection of $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_skpca = skpca.transform(X_test, Knm=Knm_test)\n",
    "\n",
    "plot_projection(Y_test, T_skpca, title=f\"Sparse KPCA on {n_active} Environments\", **cmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpca.statistics(X)` returns available statistics. Let's compare to KPCA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_kpca = KPCA(n_KPCA=n_PCA, kernel_type=kernel_type)\n",
    "ref_kpca.fit(X_train, K=K_train)\n",
    "\n",
    "table_from_dict([ref_kpca.statistics(X_train, K=K_train), \n",
    "                 skpca.statistics(X_train, Knm=Knm_train),\n",
    "                 ref_kpca.statistics(X_test, K=K_test), \n",
    "                 skpca.statistics(X_test, Knm=Knm_test)], \n",
    "                 headers = [\"KPCA (Train)\", \"Sparse KPCA(Train)\",\n",
    "                            \"KPCA (Testing)\", \"Sparse KPCA(Testing)\",\n",
    "                           ], \n",
    "                 title=\"Error in Kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse KRR with Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skrr = SparseKRR(regularization=regularization, n_active=n_active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skrr.fit(X,Y)` computes the weights $\\mathbf{P}_{KY}$ and internally stores them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skrr.fit(X_train, Y_train, Kmm=Kmm, Knm=Knm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skrr.transform(X)` computes and return the predicted $\\mathbf{Y}$ values from $\\hat{\\mathbf{Y}}_{SKRR} = \\mathbf{K}\\mathbf{P}_{KY}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_skrr_train = skrr.transform(X_train, Knm=Knm_train)\n",
    "Y_skrr_test = skrr.transform(X_test, Knm=Knm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "ref_krr = KRR(regularization=regularization)\n",
    "ref_krr.fit(X_train, Y_train, K=K_train)\n",
    "Y_krr = ref_krr.transform(X_test, K=K_test)\n",
    "\n",
    "plot_regression(Y_train[:,0], Y_skrr_train[:,0], title=\"Sparse KRR on {} Environments\".format(n_active), fig=fig, ax=axes[0], **cmaps)\n",
    "plot_regression(Y_test[:,0], Y_krr[:,0], title=\"KRR\", fig=fig, ax=axes[1], **cmaps)\n",
    "\n",
    "fig.subplots_adjust(wspace=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling `skrr.statistics(X,Y)` outputs the statistics of the regression of $\\mathbf{X}$ and $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We even compare the results of Sparse KRR with our earlier computed KRR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_from_dict([skrr.statistics(X_test, Y_test, Knm=Knm_test), \n",
    "                 ref_krr.statistics(X_test, Y_test, K=K_test)], \n",
    "                 headers = [\"Sparse KRR\", \"KRR\"], \n",
    "                 title=\"KRR Methods: Testing Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse KPCovR from the Utility Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "regularization=1e-12\n",
    "skp = SparseKPCovR(alpha=alpha, n_PCA=2, \n",
    "                   n_active=n_active, regularization=regularization, \n",
    "                   kernel_type=kernel_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skp.fit(X_train, Y_train, Knm=Knm_train, Kmm=Kmm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y, x = skp.transform(X_test, Knm=Knm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=dbl_fig)\n",
    "\n",
    "plot_projection(Y_test, t, title = \"Sparse KPCovR\", **cmaps, fig=fig, ax=axes[0])\n",
    "plot_regression(Y_test, y, title = \"Sparse KPCovR\", **cmaps, fig=fig, ax=axes[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_kpcovr = KPCovR(alpha=alpha, regularization=regularization, kernel_type=kernel_type)\n",
    "ref_kpcovr.fit(X_train, Y_train, K=K_train)\n",
    "\n",
    "table_from_dict([skp.statistics(X_test, Y_test, Knm=Knm_test),\n",
    "                ref_kpcovr.statistics(X_test, Y_test, K=K_test)],\n",
    "                headers = [\"Sparse KPCovR\", \"Full KPCovR\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.917px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
