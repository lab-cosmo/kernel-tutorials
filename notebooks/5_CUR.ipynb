{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction and Background\n",
    "\n",
    "In this notebook, we demonstrate how to use CUR matrix decomposition to provide a low-rank approximation for a matrix. [(Mahoney 2009)](https://doi.org/10.1073/pnas.0803205106), [(Wikipedia)](https://en.wikipedia.org/wiki/CUR_matrix_approximation)\n",
    "\n",
    "As should not be surprising at this point, for each model, we first go step-by-step through the derivation, with equations, embedded links, and citations supplied where useful. Lastly, we employ a \"Utility Class\" for the model, which is found in the utilities folder and contains all necessary functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Maths things\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams, cm\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Local Utilities for Notebook\n",
    "sys.path.append(\"../\")\n",
    "from utilities.general import load_variables\n",
    "from utilities.plotting import (\n",
    "    plot_simple,\n",
    "    plot_projection,\n",
    "    plot_regression,\n",
    "    check_mirrors,\n",
    "    get_cmaps,\n",
    "    table_from_dict,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Ridge as LR\n",
    "from skcosmo.feature_selection import CUR, FPS\n",
    "from skcosmo.feature_selection import PCovCUR, PCovFPS\n",
    "from skcosmo.utils import pcovr_covariance\n",
    "\n",
    "cmaps = get_cmaps()\n",
    "plt.style.use(\"../utilities/kernel_pcovr.mplstyle\")\n",
    "dbl_fig = (2 * plt.rcParams[\"figure.figsize\"][0], plt.rcParams[\"figure.figsize\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must load the data. For a step-by-step explanation of this, please see [Importing Data](X_ImportingData.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dict = load_variables(n_FPS=None)\n",
    "locals().update(var_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals of CUR\n",
    "\n",
    "PCA makes it possible to identify the directions of the maximal variance of a dataset. Finding a projection of a new point along those coordinates, however, requires computing all the features in the initial description of the problem.\n",
    "\n",
    "\n",
    "CUR decomposition, instead, attempts to find a low-[rank](https://en.wikipedia.org/wiki/Rank_(linear_algebra)) approximation of the feature matrix that is computed without having to evaluate every feature, or without using every point in the data set. Usually, the matrices that enter the decomposition are labelled $\\mathbf{C}$, $\\mathbf{U}$, and $\\mathbf{R}$, with reference to columns and rows of the initial matrix. To avoid naming conflicts with other matrices used in the rest of the tutorial, for a given matrix $\\mathbf{A}$, we use the subscripts $c$ and $r$ to denote the matrices constructed out of the _columns_ and _rows_ of $\\mathbf{A}$, and $\\mathbf{S}$ for the rectangular matrix determined by $\\mathbf{A}_c$ and $\\mathbf{A}_r$. In other words, $\\mathbf{C}\\to\\mathbf{A}_c$, $\\mathbf{R}\\to\\mathbf{A}_r$, and $\\mathbf{U}\\to\\mathbf{S}$ and\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{A} \\approx \\tilde{\\mathbf{A}} \\equiv \\mathbf{A}_c \\mathbf{S} \\mathbf{A}_r,\n",
    "\\end{equation}\n",
    "\n",
    "Many strategies have been proposed to choose the best columns and rows. We summarize a variation on the theme that was introduced in [(Imbalzano 2018)](https://doi.org/10.1063/1.5024611), that is comparatively time consuming, but deterministic and  efficient in reducing the number of features needed to approximate $\\mathbf{A}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this exercise, we compute the CUR decomposition of our training set $\\mathbf{X}$. \n",
    "Because we later use CUR to choose features, we omitted the step of furthest point sampling in loading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Columns\n",
    "\n",
    "The CUR algorithm we implement modifies in-place the feature matrix. For this reason, we use a copy to perform the column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nCUR = 50\n",
    "X_copy = X_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $c$ columns and $r$ rows are chosen based upon their \"influence\" on the overall matrix. We compute this \"influence\" using a **normalized statistical leverage score** $\\pi$  computed from the [**singular value decomposition**](https://en.wikipedia.org/wiki/Singular_value_decomposition): $\\mathbf{X} = \\mathbf{U}_K\\mathbf{\\Sigma U}_C^T$, where $\\mathbf{U_K}$ and $\\mathbf{U_C}$ are the right and left principal components, respectively, and coincide with the eigenvectors of the Gram matrix $\\mathbf{K} = \\mathbf{XX}^T$ and the covariance $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X}$, also respectively. $\\mathbf{\\Sigma}$ is a rectangular-diagonal matrix with the singular values, which equal the square roots of the non-zero eigenvalues of both $\\mathbf{C}$ or $\\mathbf{K}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "(U_K, sig, U_C) = np.linalg.svd(X_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it helps (depending on the size of the matrix!) to use the sparse SVD from scipy.sparse to compute only $k$ components of the SVD, where $k$ is the target rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=1\n",
    "\n",
    "%time\n",
    "(U_K, sig, U_C) = sps.linalg.svds(X_copy,k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the $j^{th}$ column, the importance score is given by\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_j = \\sum_i^k (U_C)_{ji}^2,\n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is again the target rank of the new matrix and $(U_C)_{ji}$ is the value in the $j^{th}$ row and $i^{th}$ column of $\\mathbf{U}_C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = (U_C[:k] ** 2.0).sum(axis=0)\n",
    "j = pi.argmax()\n",
    "\n",
    "plt.plot(pi)\n",
    "plt.scatter(j, max(pi))\n",
    "plt.xlabel(\"j\")\n",
    "plt.ylabel(r\"$\\pi_j$\")\n",
    "plt.title(f\"Column {j} is most influential.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonalizing the Rest of the Columns\n",
    "\n",
    "Most CUR algorithms select elements in a stochastic manner based on the statistical leverage score. Here we select in a deterministic manner the most influential feature. \n",
    "After choosing the $j^{th}$ column, we must orthogonalize the remaining $j'$ columns with respect to the $j^{th}$ column that has just been selected and removed. This way we make sure that redundant entries won't be chosen over and over.\n",
    "\n",
    "\\begin{equation}\n",
    "X_{j'} = X_{j'} - X_{j}\\left(\\frac{\\mathbf{X}_j\\cdot \\mathbf{X}_{j'}}{\\mathbf{X}_j\\cdot \\mathbf{X}_{j}}\\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = X_copy[:, j] / np.sqrt(X_copy[:, j] @ X_copy[:, j])\n",
    "\n",
    "for i in range(X_copy.shape[1]):\n",
    "    X_copy[:, i] -= v * np.dot(v, X_copy[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can repeat the procedure several times, until we've collected enough entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [j]\n",
    "\n",
    "for n in tqdm(range(nCUR - 1)):\n",
    "    (U_K, sig, U_C) = sps.linalg.svds(X_copy, k)\n",
    "    pi = (U_C[:k] ** 2.0).sum(axis=0)\n",
    "    idxs.append(pi.argmax())\n",
    "\n",
    "    v = X_copy[:, idxs[-1]] / np.sqrt(X_copy[:, idxs[-1]] @ X_copy[:, idxs[-1]])\n",
    "\n",
    "    for i in range(X_copy.shape[1]):\n",
    "        X_copy[:, i] -= v * np.dot(v, X_copy[:, i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our column indices and construct $\\mathbf{X}_C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.asarray(idxs)\n",
    "\n",
    "X_c = X_train[:, idxs]\n",
    "X_c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Rows\n",
    "If the matrix we are decomposing is symmetric, we can use the same indices for the columns and rows. If it is non-symmetric (such as $\\mathbf{X}$ is here), the $r$ most influential rows are found by computing the $r$ most influential columns of the transpose of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs_r = []\n",
    "X_T = X_train.copy().T\n",
    "\n",
    "for n in tqdm(range(nCUR)):\n",
    "    (U_K, sig, U_C) = sps.linalg.svds(X_T, k)\n",
    "    pi = (U_C[:k] ** 2.0).sum(axis=0)\n",
    "    idxs_r.append(pi.argmax())\n",
    "\n",
    "    v = X_T[:, idxs_r[-1]] / np.sqrt(X_T[:, idxs_r[-1]] @ X_T[:, idxs_r[-1]])\n",
    "\n",
    "    for i in range(X_T.shape[1]):\n",
    "        X_T[:, i] -= v * np.dot(v, X_T[:, i])\n",
    "\n",
    "X_r = X_train[idxs_r]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One determines $\\mathbf{S}$ by computing the [pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse) of $\\mathbf{X}_c$ and $\\mathbf{X}_r$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{S}  = \\mathbf{X}_c^- \\mathbf{X} \\mathbf{X}_r^- =  (\\mathbf{X}_c^T \\mathbf{X}_c)^{-1} \\mathbf{X}_c^T \\mathbf{X}(\\mathbf{X}_r^T\\mathbf{X}_r)^{-1}  \\mathbf{X}_r^T .\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_c = np.linalg.pinv(X_c)\n",
    "S_r = np.linalg.pinv(X_r)\n",
    "\n",
    "S = S_c @ X_train @ S_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximate $\\mathbf{\\tilde{X}}$ is the same shape as $\\mathbf{X}$, but is low-rank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt = X_c @ S @ X_r\n",
    "\n",
    "print(np.linalg.matrix_rank(Xt), np.linalg.matrix_rank(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and approximates $\\mathbf{X}$, in a way that can be improved by increasing the number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(Xt - X_train) / np.linalg.norm(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def approx_X(A, col_idx, row_idx=None):\n",
    "    \"\"\" Approximates the full matrix with selected features \"\"\"\n",
    "\n",
    "    A_c = A[:, col_idx]\n",
    "    S_c = np.linalg.pinv(A_c)\n",
    "\n",
    "    A_r = A[row_idx]\n",
    "\n",
    "    if row_idx is not None:\n",
    "        S_r = np.linalg.pinv(A_r)\n",
    "        S = S_c @ A @ S_r\n",
    "    else:\n",
    "        S = S_c\n",
    "\n",
    "    return A_c @ S @ A_r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUR for Feature Selection\n",
    "\n",
    "If one chooses to use CUR to select columns/features (analogous to how we have used FPS), the row matrix $\\mathbf{X}_r$ is just the full $\\mathbf{X}$, and so one determines $\\mathbf{S}$ by computing the pseudoinverse of $\\mathbf{X}_c$ and $\\mathbf{X}$,\n",
    "\\begin{equation}\n",
    "\\mathbf{S}  = \\mathbf{X}_c^- \\mathbf{X} \\mathbf{X}^-\n",
    "=(\\mathbf{X}_c^T \\mathbf{X}_c)^{-1} \\mathbf{X}_c^T \\mathbf{X} (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_c = np.linalg.pinv(X_c)\n",
    "S_X = np.linalg.pinv(X_train)\n",
    "\n",
    "S = S_c @ X_train @ S_X\n",
    "\n",
    "Xt_raw = X_c @ S @ X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For feature selection $\\tilde{\\mathbf{X}}$ reduces to:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{\\mathbf{X}} &= \\mathbf{X}_c(\\mathbf{X}_c^T \\mathbf{X}_c)^{-1} \\mathbf{X}_c^T \\mathbf{X} (\\mathbf{X}^T\\mathbf{X})^{-1} \\mathbf{X}^T\\mathbf{X}\\\\\n",
    "\\tilde{\\mathbf{X}} &= \\mathbf{X}_c(\\mathbf{X}_c^T \\mathbf{X}_c)^{-1} \\mathbf{X}_c^T \\mathbf{X}\n",
    "\\end{align}\n",
    "\n",
    "so $\\mathbf{S}$ is instead computed as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{S} = (\\mathbf{X}_c^T \\mathbf{X}_c)^{-1} \\mathbf{X}_c^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.linalg.pinv(X_c)\n",
    "Xt = X_c @ S @ X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(Xt_raw - Xt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the utility `approx_A(X_train, idx)` in `utilities/CUR.py` to do this process from now on. \n",
    "\n",
    "With this dataset (but not all datasets), CUR minimizes the loss compared to FPS, especially for small numbers of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = np.array([int(10 ** x) for x in np.linspace(1, np.log10(nCUR), 20)])\n",
    "ifps = FPS(n_to_select=n).fit(X_train).selected_idx_\n",
    "\n",
    "X_CUR = [approx_X(X_train, idxs[:n]) for n in ns]\n",
    "X_FPS = [approx_X(X_train, ifps[:n]) for n in ns]\n",
    "\n",
    "err_CUR = np.array(\n",
    "    [np.linalg.norm(X_train - Xt) / np.linalg.norm(X_train) for Xt in X_CUR]\n",
    ")\n",
    "err_FPS = np.array(\n",
    "    [np.linalg.norm(X_train - Xt) / np.linalg.norm(X_train) for Xt in X_FPS]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=dbl_fig)\n",
    "\n",
    "plt.loglog(ns, err_CUR, marker=\"o\", label=\"CUR\", c=\"r\")\n",
    "plt.loglog(ns, err_FPS, marker=\"o\", label=\"FPS\", c=\"b\")\n",
    "\n",
    "ax.set_xlabel(\"# Columns\")\n",
    "ax.set_ylabel(r\"$\\frac{|X-\\tilde{X}|}{|X|}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the Approximation in ML Models\n",
    "\n",
    "## Projecting into Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whenever the features are used to approximate distances or scalar products, it is possible to obtain a $n_{samples}\\times n_{CUR}$ reduced feature matrix $\\mathbf{T} = \\mathbf{X}_c \\mathbf{P}_{X_c T}$ that yields the exact same values as when using the approximate $\\tilde{\\mathbf{X}}$. For the sake of brevity, we abbreviate $\\mathbf{P}_{X_c T}$ to $\\mathbf{P}_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<!---**Logic 1** We start with the equation for projecting $\\mathbf{X}$ into latent space via PCA:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{X}\\hat{\\mathbf{U}}_{C}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X}$, and $\\mathbf{U}_C$ is truncated to some value of $n_{PCA}$. We replace $\\mathbf{X}$ with its feature-selected approximation $\\tilde{\\mathbf{X}}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{T} = \\mathbf{X}_c\\mathbf{SX}_r\\mathbf{U}_{C}.\n",
    "\\end{equation}\n",
    "\n",
    "Therefore $\\mathbf{P}_{c} = \\mathbf{SX}_r\\hat{\\mathbf{U}}_{C}$. To avoid explicitly calculating $\\mathbf{U}_{C}$, we instead take advantage of its orthogonality and solve for $\\mathbf{P}_{c}$ from\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{P}_{c}\\mathbf{P}_{c}^T &=\\mathbf{SX}_r\\mathbf{U}_{C}\\mathbf{U}_{C}^T\\mathbf{X}_r^T\\mathbf{S}^T\\\\\n",
    "&=\\mathbf{SX}_r\\mathbf{X}_r^T\\mathbf{S}^T\\\\\n",
    "\\end{align} -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is to compute $\\mathbf{T}$ such that $\\mathbf{TT}^T$ is identical to $\\mathbf{\\tilde{X}\\tilde{X}}^T$. Given the definition of $\\mathbf{\\tilde{X}}$, we get \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\tilde{X}\\tilde{X}}^T = \\mathbf{X}_c\\mathbf{SX}_r\\mathbf{X}_r^T\\mathbf{S}^T\\mathbf{X}_c^T=\n",
    "\\mathbf{X}_c \\mathbf{P}_c\\mathbf{P}_c^T \\mathbf{X}_c^T = \\mathbf{TT}^T\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which yields the symmytric solution\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{P}_{c} = \\left[ \\mathbf{SX}_r\\mathbf{X}_r^T\\mathbf{S} \\right]^{1/2}. \\label{eq:cur-t}\n",
    "\\end{equation}\n",
    "\n",
    "The $n_{CUR} \\times n_{CUR}$ matrix $\\mathbf{P}_{c}$ is computed once and re-used every time one needs to compute $\\mathbf{T}$ for new samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_c = np.linalg.pinv(X_c)\n",
    "S_r = np.linalg.pinv(X_r)\n",
    "\n",
    "S = S_c @ X_train @ S_r\n",
    "\n",
    "SX = S @ X_r\n",
    "SXXS = SX @ SX.T\n",
    "\n",
    "v_SX, U_SX = np.linalg.eigh(SXXS)\n",
    "v_SX[v_SX < 1e-12] = 0\n",
    "\n",
    "P_c = U_SX @ np.diagflat(np.sqrt(v_SX))\n",
    "\n",
    "T = X_c @ P_c\n",
    "T_test = X_test[:, idxs] @ P_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, we use the  utility function `compute_P(X_c, S, X_r)` in `utilities/CUR.py` to do this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_P(A_c, S, A_r, thresh=1e-12):\n",
    "    \"\"\" Computes the latent-space projector for the feature matrix \"\"\"\n",
    "    SA = S @ A_r\n",
    "    SA = SA @ SA.T\n",
    "\n",
    "    v_SA, U_SA = np.linalg.eigh(SA)\n",
    "    v_SA[v_SA < thresh] = 0\n",
    "\n",
    "    return U_SA @ np.diagflat(np.sqrt(v_SA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUR latent space approximates very well the principal components of the feature space. The eigenvalues of the latent-space covariance match those of the full covariance. Note also how the scaling by $\\mathbf{P}_c$ of $\\mathbf{X}_c$ improves dramatically the match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, sharex=True, figsize=(10, 12))\n",
    "\n",
    "eigTTT = np.array(list(reversed(np.linalg.eigvalsh(T.T @ T))))\n",
    "eigXcTXc = np.array(list(reversed(np.linalg.eigvalsh(X_c.T @ X_c)[-T.shape[1] :])))\n",
    "eigXTX = np.array(\n",
    "    list(reversed(np.linalg.eigvalsh(X_train.T @ X_train)[-T.shape[1] :]))\n",
    ")\n",
    "\n",
    "ax[0].semilogy(eigTTT, \"r\", label=r\"$\\lambda_{T^T T}$\")\n",
    "ax[0].semilogy(eigXcTXc, \"r--\", label=r\"$\\lambda_{X_c^T X_c}$\")\n",
    "ax[0].semilogy(eigXTX, \"b\", label=r\"$\\lambda_{X^T X}$\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[0].set_ylabel(r\"$\\lambda_n$\", fontsize=20)\n",
    "\n",
    "error = [abs(t - a) / a for t, a in zip(eigTTT, eigXTX)]\n",
    "ax[1].semilogy(error, marker=\"o\")\n",
    "ax[1].set_ylabel(\n",
    "    r\"$\\frac{|\\lambda_{X^T X}-\\lambda_{T^T T}|}{|\\lambda_{X^T X}|}$\", fontsize=20\n",
    ")\n",
    "ax[1].set_xlabel(\"n\", fontsize=20)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A measure of error is given by the accuracy in representing the Gram matrix (the Torgenson loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Torgenson loss with raw column selection\", \n",
    "      np.linalg.norm(X_train @ X_train.T - X_c @ X_c.T))\n",
    "print(\"Torgenson loss with scaled, CUR latent space\", \n",
    "      np.linalg.norm(X_train @ X_train.T - T @ T.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the projections and errors with the original $\\mathbf{X}$ and the CUR feature-selected $\\mathbf{\\tilde{X}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_high_rank = PCA(n_components=2)\n",
    "pca_high_rank.fit(X_train)\n",
    "T_high_rank = pca_high_rank.transform(X_test)\n",
    "\n",
    "S = np.linalg.pinv(X_c)\n",
    "P = compute_P(X_train[:, idxs], S, X_train)\n",
    "\n",
    "# we find the CUR latent space for train and test\n",
    "T_train = X_train[:, idxs] @ P\n",
    "T_test = X_test[:, idxs] @ P\n",
    "\n",
    "pca_low_rank = PCA(n_components=2)\n",
    "pca_low_rank.fit(T_train)\n",
    "T_low_rank = pca_low_rank.transform(T_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projections based on the CUR selected features reproduce well those based on the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "\n",
    "plot_projection(Y=Y_test[:,0], T=T_high_rank, fig=fig, ax=ax[0], \n",
    "                title=\"Projection using Original Data\", \n",
    "                **cmaps)\n",
    "plot_projection(Y=Y_test[:,0], T=check_mirrors(T_low_rank,T_high_rank), \n",
    "                fig=fig, ax=ax[1],  \n",
    "                title=\"Projection using Low-Rank Approximation\", \n",
    "                **cmaps)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA statistics are not particularly meaningful, because they report on the performance of PCA in representing the full or CUR-reduced feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is more meaningful to compare the performance of a regression model built on the full $\\mathbf{X}$ or on the latent CUR space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_high_rank = LR(alpha=1e-6)\n",
    "lr_high_rank.fit(X_train, Y_train)\n",
    "Ylr_high_rank = lr_high_rank.predict(X_test)\n",
    "\n",
    "lr_low_rank = LR(alpha=1e-6)\n",
    "lr_low_rank.fit(T_train, Y_train)\n",
    "Ylr_low_rank = lr_low_rank.predict(T_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "fig, ax = plt.subplots(1,2, figsize=(12,5))\n",
    "\n",
    "Yerr = np.abs(Y_test-Ylr_high_rank)\n",
    "plot_regression(Y=Y_test[:,0], Yp=Ylr_high_rank[:,0], \n",
    "                fig=fig, ax=ax[0], \n",
    "                title=\"Regression using Original Data\", \n",
    "                vmin=Yerr.min(), vmax=Yerr.max(),\n",
    "                **cmaps)\n",
    "plot_regression(Y=Y_test[:,0], Yp=Ylr_low_rank[:,0],\n",
    "                fig=fig, ax=ax[1],  \n",
    "                vmin=Yerr.min(), vmax=Yerr.max(),\n",
    "                title=\"Regression using Low-Rank Approximation\", \n",
    "                **cmaps)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need for Orthogonalizing\n",
    "\n",
    "Now that we know how to compute the error of each approximation, we look at the importance of the orthogonalization procedure discussed in Sec. 2.2. \n",
    "\n",
    "Imagine we were to choose all of our columns at once, corresponding to the top $n_{CUR}$ features based on their leverage score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(U_K, sig, U_C) = np.linalg.svd(X_train)\n",
    "pi = (U_C[:k]**2.0).sum(axis=0)\n",
    "\n",
    "idxs_oneoff = np.flip(np.argsort(pi))[:nCUR]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, as is typically done with CUR, we iteratively choose our columns, but without the step of orthogonalizing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X_train.copy()\n",
    "\n",
    "idxs_no_orth=[]\n",
    "for n in tqdm(range(nCUR)):\n",
    "    (U_K, sig, U_C) = sps.linalg.svds(X_copy,k)\n",
    "    pi = (U_C[:k]**2.0).sum(axis=0)\n",
    "\n",
    "    idxs_no_orth.append(pi.argmax())\n",
    "    X_copy[:,idxs_no_orth]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we compare the Gram matrix approximation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_oneoff = np.linalg.pinv(X_train[:, idxs_oneoff]) @ X_train @ np.linalg.pinv(X_train)\n",
    "P_oneoff = compute_P(X_train[:, idxs_oneoff], S_oneoff, X_train)\n",
    "T_train_oneoff = X_train[:, idxs_oneoff] @ P_oneoff\n",
    "T_test_oneoff = X_test[:, idxs_oneoff] @ P_oneoff\n",
    "\n",
    "S_no_orth = np.linalg.pinv(X_train[:, idxs_no_orth]) @ X_train @ np.linalg.pinv(X_train)\n",
    "P_no_orth = compute_P(X_train[:, idxs_no_orth], S_no_orth, X_train)\n",
    "T_train_no_orth = X_train[:, idxs_no_orth] @ P_no_orth\n",
    "T_test_no_orth = X_test[:, idxs_no_orth] @ P_no_orth\n",
    "\n",
    "print(\"Torgenson loss with one-off leverage score selection\", \n",
    "      np.linalg.norm(X_train @ X_train.T-T_train_oneoff@T_train_oneoff.T))\n",
    "\n",
    "print(\"Torgenson loss with iterative selection, no orthogonalisation\", \n",
    "      np.linalg.norm(X_train@X_train.T-T_train_no_orth@T_train_no_orth.T))\n",
    "\n",
    "print(\"Torgenson loss with iterative orthogonalisation\", \n",
    "      np.linalg.norm(X_train@X_train.T-T_train@T_train.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCov-CUR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The New Leverage Scores\n",
    "\n",
    "The PCovR formulation is particularly appealing to select a small number of ML features to be used for machine learning, because it allows incorporating information on the ability of different features to predict the target properties.\n",
    "We propose to proceed as in Section 3, but to compute the leverage scores by computing the eigenvectors of the PCovR covariance matrix $\\mathbf{\\tilde{C}}$ in place of the singular value decomposition, given by \n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{\\mathbf{C}} = \\alpha \\mathbf{C}\n",
    "    + (1 - \\alpha)  \\mathbf{C}^{1/2}\\left(\\mathbf{C}+\\lambda \\mathbf{I}\\right)^{-1}\\mathbf{X}^T{\\mathbf{Y} \\mathbf{Y}^T}\\mathbf{X}\\left(\\mathbf{C}+\\lambda \\mathbf{I}\\right)^{-1}\\mathbf{C}^{1/2}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{C} = \\mathbf{X}^T\\mathbf{X}$. The best results are obtained by computing leverage scores using a number of eigenvectors that is smaller or equal than the number of properties in $\\mathbf{Y}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have to calculate $\\mathbf{\\tilde{C}}$ iteratively, we use a utility function `get_Ct(X, Y, alpha)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_copy = X_train.copy()\n",
    "Y_copy = Y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "alpha = 0.5\n",
    "nCUR = 10\n",
    "Ct = pcovr_covariance(X=X_copy, Y=Y_copy, mixing=alpha, rcond=1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_Ct, U_Ct = sps.linalg.eigs(Ct, k=1)\n",
    "U_Ct = U_Ct[:, np.argsort(-v_Ct)]\n",
    "v_Ct = v_Ct[np.argsort(-v_Ct)]\n",
    "\n",
    "pi = (U_Ct[:,:k]**2.0).sum(axis=1)\n",
    "j = pi.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pi)\n",
    "plt.scatter(j, max(pi))\n",
    "plt.xlabel(\"j\")\n",
    "plt.ylabel(r\"$\\pi_j$\")\n",
    "plt.title(f\"Column {j} is most influential.\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_c = X_copy[:, [j]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting the Remaining Columns\n",
    "\n",
    "The only additional change that needs to be incorporated in the procedure described in Sec. 2.2 involves eliminating at each step the components of the property matrix $\\mathbf{Y}$ that is described by the features that have already been selected and removed. Computing at each step the reduced vector $\\mathbf{T} = \\mathbf{X}_c\\mathbf{P}_{c}$, one should perform the update \n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{Y} = \\mathbf{Y} - \\mathbf{T} \\left(\\mathbf{T}^T \\mathbf{T}\\right)^{-1}\\mathbf{T}^T \\mathbf{Y}.\n",
    "\\end{equation}\n",
    "\n",
    "Note that when the linear regression is performed without regularisation, the transformation from $\\mathbf{X}_c$ to $\\mathbf{T}$ is inconsequential (as it can be seen just by cancelling out instances of $\\mathbf{P}_{c}$, which one can do when $\\mathbf{P}_{c}$ is invertible and symmetric). We can then avoid the calculation of the projection matrix and simply do\n",
    "\n",
    "\\begin{equation}\n",
    "    \\mathbf{Y} = \\mathbf{Y} - \\mathbf{X}_c \\left(\\mathbf{X}_c^T \\mathbf{X}_c\\right)^{-1}\\mathbf{X}_c^T \\mathbf{Y}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.linalg.pinv(X_c)\n",
    "\n",
    "SX = S @ X_copy\n",
    "SX = SX @ SX.T \n",
    "\n",
    "v_SX, U_SX = np.linalg.eigh(SX) \n",
    "v_SX[v_SX<1e-12] = 0\n",
    "\n",
    "P_c = U_SX @ np.diagflat(np.sqrt(v_SX))\n",
    "\n",
    "T = X_c @ P_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the two vectors are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = np.linalg.pinv(T.T @ T)\n",
    "v1 = T @ v1\n",
    "v1 = v1 @ T.T\n",
    "\n",
    "v2 = np.linalg.pinv(X_c.T @ X_c)\n",
    "v2 = X_c @ v2\n",
    "v2 = v2 @ X_c.T\n",
    "print(np.linalg.norm(v1-v2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_copy -= v2 @ Y_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like in Sec. 2.2, we must orthogonalize the remaining $j'$ columns in $\\mathbf{X}$ with respect to the $j^{th}$ column that has just been removed.\n",
    "\n",
    "\\begin{equation}\n",
    "X_{kj'} = X_{kj'} - X_{kj}\\left(\\frac{\\mathbf{X}_j\\cdot \\mathbf{X}_{j'}}{\\mathbf{X}_j\\cdot \\mathbf{X}_{j}}\\right).\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = X_copy[:,j]/np.sqrt(X_copy[:, j] @ X_copy[:, j])\n",
    "\n",
    "for i in range(X_copy.shape[1]):\n",
    "    X_copy[:,i] -= v * np.dot(v,X_copy[:,i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the Remaining Columns\n",
    "\n",
    "We then iterate until we've chosen the right number of columns (beware: this can be slow!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = [j]\n",
    "\n",
    "for n in tqdm(range(nCUR-1)):\n",
    "    \n",
    "    try:\n",
    "        Ct = pcovr_covariance(X=X_copy, Y=Y_copy, mixing=alpha, rcond=1e-7)\n",
    "    except:\n",
    "        print(f\"Only {n} features possible\")\n",
    "        break\n",
    "        \n",
    "    v_Ct, U_Ct = sps.linalg.eigs(Ct, k=1)\n",
    "    U_Ct = U_Ct[:, np.argsort(-v_Ct)]\n",
    "    v_Ct = v_Ct[np.argsort(-v_Ct)]\n",
    "    \n",
    "    pi = (U_Ct[:,:k]**2.0).sum(axis=1)\n",
    "    \n",
    "    j=pi.argmax()\n",
    "    idxs.append(j)\n",
    "    \n",
    "    X_c = X_copy[:, idxs]\n",
    "    v = np.linalg.pinv(X_c.T @ X_c)\n",
    "    v = X_c @ v\n",
    "    v = v @ X_c.T\n",
    "\n",
    "    Y_copy -= v @ Y_copy\n",
    "    \n",
    "    v = X_copy[:,j]/np.sqrt(X_copy[:, j] @ X_copy[:, j])\n",
    "\n",
    "    for i in range(X_copy.shape[1]):\n",
    "        X_copy[:,i] -= v * np.dot(v,X_copy[:,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.asarray(idxs)\n",
    "idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do the usual gig to find the CUR latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = np.linalg.pinv(X_train[:,idxs])\n",
    "P = compute_P(X_train[:, idxs], S, X_train)\n",
    "\n",
    "# we find the CUR latent space for train and test\n",
    "T_pcov = X_train[:, idxs] @ P\n",
    "T_pcov_test = X_test[:, idxs] @ P\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and show how choosing columns with a PCovR-based leverage score improve substantially the regression performance (note that you must have run the previous cells in the notebook to see the stats from the vanilla CUR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pcov = LR(alpha=1e-6)\n",
    "lr_pcov.fit(T_pcov, Y_train)\n",
    "Ylr_pcov = lr_pcov.predict(T_pcov_test)\n",
    "\n",
    "plot_regression(Y_test[:,0], Ylr_pcov[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Utility Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUR Framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = CUR(n_to_select=max(ns), progress_bar=True)\n",
    "idx_cur = cur.fit(X_train).selected_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = [np.linalg.norm(X_test - approx_X(X_test, idx_cur[:n]))**2.0 / np.linalg.norm(X_test)**2 for n in tqdm(ns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple(np.array([ns, errors]).T, **cmaps)\n",
    "plt.xlabel(r\"$n_{CUR}$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\frac{\\left| X - \\tilde{X} \\right|}{| X |}$\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCov-CUR\n",
    "\n",
    "If you want to compute your importance scores using $\\mathbf{U}_{\\tilde{C}}$, you can provide the class `\\mixing < 1` and a set of properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = PCovCUR(n_to_select=max(ns), mixing=0.5, progress_bar=True)\n",
    "idx_pcov = cur.fit(X=X_train, y=Y_train).selected_idx_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_pcovs = [\n",
    "    np.linalg.norm(X_test - approx_X(X_test, idx_pcov[:n])) ** 2.0\n",
    "    / np.linalg.norm(X_test) ** 2\n",
    "    for n in tqdm(ns)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simple(np.array([ns, error_pcovs]).T, **cmaps)\n",
    "plt.xlabel(r\"$n_{CUR}$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\frac{\\left| X - \\tilde{X} \\right|}{| X |}$\", fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "def regression_error(idx):\n",
    "    ridge = RidgeCV(fit_intercept=False, alphas = np.logspace(-8,-4,10), cv=2)\n",
    "    ridge.fit(X_train[:, idx], Y_train)\n",
    "    return np.linalg.norm(Y_test - ridge.predict(X_test[:, idx]))**2.0 / np.linalg.norm(Y_test)**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_regr_pcovs = [\n",
    "    regression_error(idx_pcov[:n])\n",
    "    for n in tqdm(ns)\n",
    "]\n",
    "error_regr_cur = [\n",
    "    regression_error(idx_cur[:n])\n",
    "    for n in tqdm(ns)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.loglog(ns, error_regr_pcovs, c='b', label='PCov-CUR')\n",
    "plt.loglog(ns, error_regr_cur, c='r', label='Classic CUR')\n",
    "plt.xlabel(r\"$n_{CUR}$\", fontsize=20)\n",
    "plt.ylabel(r\"$\\frac{\\left| Y - \\hat{Y} \\right|}{| Y |}$\", fontsize=20)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "210px",
    "width": "289px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.917px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
